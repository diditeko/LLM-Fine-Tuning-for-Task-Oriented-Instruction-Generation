{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 34200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014623089858887182,
      "grad_norm": 1.1993589401245117,
      "learning_rate": 0.00019971345029239768,
      "loss": 1.7043,
      "step": 50
    },
    {
      "epoch": 0.029246179717774365,
      "grad_norm": 1.608686089515686,
      "learning_rate": 0.00019942105263157895,
      "loss": 1.4962,
      "step": 100
    },
    {
      "epoch": 0.04386926957666155,
      "grad_norm": 1.0142511129379272,
      "learning_rate": 0.00019912865497076027,
      "loss": 1.4267,
      "step": 150
    },
    {
      "epoch": 0.05849235943554873,
      "grad_norm": 0.5814701318740845,
      "learning_rate": 0.00019883625730994153,
      "loss": 1.4811,
      "step": 200
    },
    {
      "epoch": 0.07311544929443592,
      "grad_norm": 0.9088965654373169,
      "learning_rate": 0.0001985438596491228,
      "loss": 1.4254,
      "step": 250
    },
    {
      "epoch": 0.0877385391533231,
      "grad_norm": 0.9030502438545227,
      "learning_rate": 0.00019825146198830412,
      "loss": 1.4186,
      "step": 300
    },
    {
      "epoch": 0.10236162901221028,
      "grad_norm": 0.9654903411865234,
      "learning_rate": 0.0001979590643274854,
      "loss": 1.3949,
      "step": 350
    },
    {
      "epoch": 0.11698471887109746,
      "grad_norm": 0.9237567186355591,
      "learning_rate": 0.00019766666666666666,
      "loss": 1.4044,
      "step": 400
    },
    {
      "epoch": 0.13160780872998465,
      "grad_norm": 0.968429684638977,
      "learning_rate": 0.00019737426900584798,
      "loss": 1.4403,
      "step": 450
    },
    {
      "epoch": 0.14623089858887184,
      "grad_norm": 0.6791698336601257,
      "learning_rate": 0.00019708187134502924,
      "loss": 1.3265,
      "step": 500
    },
    {
      "epoch": 0.160853988447759,
      "grad_norm": 0.6108433604240417,
      "learning_rate": 0.00019678947368421054,
      "loss": 1.5119,
      "step": 550
    },
    {
      "epoch": 0.1754770783066462,
      "grad_norm": 0.7694073915481567,
      "learning_rate": 0.00019649707602339183,
      "loss": 1.4564,
      "step": 600
    },
    {
      "epoch": 0.19010016816553338,
      "grad_norm": 0.9861963391304016,
      "learning_rate": 0.0001962046783625731,
      "loss": 1.4229,
      "step": 650
    },
    {
      "epoch": 0.20472325802442057,
      "grad_norm": 0.7702569365501404,
      "learning_rate": 0.0001959122807017544,
      "loss": 1.4056,
      "step": 700
    },
    {
      "epoch": 0.21934634788330773,
      "grad_norm": 0.8508201241493225,
      "learning_rate": 0.0001956198830409357,
      "loss": 1.3455,
      "step": 750
    },
    {
      "epoch": 0.23396943774219492,
      "grad_norm": 0.7723119258880615,
      "learning_rate": 0.00019532748538011695,
      "loss": 1.4083,
      "step": 800
    },
    {
      "epoch": 0.2485925276010821,
      "grad_norm": 0.5762796998023987,
      "learning_rate": 0.00019503508771929825,
      "loss": 1.4337,
      "step": 850
    },
    {
      "epoch": 0.2632156174599693,
      "grad_norm": 0.5678327083587646,
      "learning_rate": 0.00019474269005847954,
      "loss": 1.4771,
      "step": 900
    },
    {
      "epoch": 0.27783870731885646,
      "grad_norm": 0.779932975769043,
      "learning_rate": 0.00019445029239766084,
      "loss": 1.3954,
      "step": 950
    },
    {
      "epoch": 0.2924617971777437,
      "grad_norm": 0.7191358208656311,
      "learning_rate": 0.00019415789473684213,
      "loss": 1.3241,
      "step": 1000
    },
    {
      "epoch": 0.30708488703663084,
      "grad_norm": 0.5171436071395874,
      "learning_rate": 0.0001938654970760234,
      "loss": 1.4212,
      "step": 1050
    },
    {
      "epoch": 0.321707976895518,
      "grad_norm": 0.46858537197113037,
      "learning_rate": 0.0001935730994152047,
      "loss": 1.4267,
      "step": 1100
    },
    {
      "epoch": 0.3363310667544052,
      "grad_norm": 1.1970490217208862,
      "learning_rate": 0.00019328070175438598,
      "loss": 1.4167,
      "step": 1150
    },
    {
      "epoch": 0.3509541566132924,
      "grad_norm": 0.8030809164047241,
      "learning_rate": 0.00019298830409356725,
      "loss": 1.3611,
      "step": 1200
    },
    {
      "epoch": 0.3655772464721796,
      "grad_norm": 0.6547696590423584,
      "learning_rate": 0.00019269590643274855,
      "loss": 1.3392,
      "step": 1250
    },
    {
      "epoch": 0.38020033633106676,
      "grad_norm": 0.6629003286361694,
      "learning_rate": 0.00019240350877192984,
      "loss": 1.4039,
      "step": 1300
    },
    {
      "epoch": 0.3948234261899539,
      "grad_norm": 0.7033646702766418,
      "learning_rate": 0.00019211111111111113,
      "loss": 1.3339,
      "step": 1350
    },
    {
      "epoch": 0.40944651604884114,
      "grad_norm": 0.970331072807312,
      "learning_rate": 0.0001918187134502924,
      "loss": 1.3999,
      "step": 1400
    },
    {
      "epoch": 0.4240696059077283,
      "grad_norm": 0.5315206050872803,
      "learning_rate": 0.0001915263157894737,
      "loss": 1.4909,
      "step": 1450
    },
    {
      "epoch": 0.43869269576661546,
      "grad_norm": 0.757258951663971,
      "learning_rate": 0.000191233918128655,
      "loss": 1.3927,
      "step": 1500
    },
    {
      "epoch": 0.4533157856255027,
      "grad_norm": 0.43507105112075806,
      "learning_rate": 0.00019094152046783626,
      "loss": 1.3884,
      "step": 1550
    },
    {
      "epoch": 0.46793887548438984,
      "grad_norm": 0.6908566355705261,
      "learning_rate": 0.00019064912280701755,
      "loss": 1.3925,
      "step": 1600
    },
    {
      "epoch": 0.48256196534327706,
      "grad_norm": 0.3415403962135315,
      "learning_rate": 0.00019035672514619884,
      "loss": 1.3838,
      "step": 1650
    },
    {
      "epoch": 0.4971850552021642,
      "grad_norm": 0.7445135712623596,
      "learning_rate": 0.0001900643274853801,
      "loss": 1.4388,
      "step": 1700
    },
    {
      "epoch": 0.5118081450610514,
      "grad_norm": 0.6297876834869385,
      "learning_rate": 0.00018977192982456143,
      "loss": 1.4112,
      "step": 1750
    },
    {
      "epoch": 0.5264312349199386,
      "grad_norm": 1.0654176473617554,
      "learning_rate": 0.0001894795321637427,
      "loss": 1.4131,
      "step": 1800
    },
    {
      "epoch": 0.5410543247788258,
      "grad_norm": 0.9346612095832825,
      "learning_rate": 0.000189187134502924,
      "loss": 1.4383,
      "step": 1850
    },
    {
      "epoch": 0.5556774146377129,
      "grad_norm": 0.7936082482337952,
      "learning_rate": 0.0001888947368421053,
      "loss": 1.3775,
      "step": 1900
    },
    {
      "epoch": 0.5703005044966001,
      "grad_norm": 0.6585286855697632,
      "learning_rate": 0.00018860233918128655,
      "loss": 1.4676,
      "step": 1950
    },
    {
      "epoch": 0.5849235943554874,
      "grad_norm": 0.5617483854293823,
      "learning_rate": 0.00018830994152046785,
      "loss": 1.3511,
      "step": 2000
    },
    {
      "epoch": 0.5995466842143745,
      "grad_norm": 0.6522823572158813,
      "learning_rate": 0.0001880233918128655,
      "loss": 1.3968,
      "step": 2050
    },
    {
      "epoch": 0.6141697740732617,
      "grad_norm": 0.7570253014564514,
      "learning_rate": 0.0001877309941520468,
      "loss": 1.3239,
      "step": 2100
    },
    {
      "epoch": 0.6287928639321488,
      "grad_norm": 0.8518516421318054,
      "learning_rate": 0.00018743859649122808,
      "loss": 1.423,
      "step": 2150
    },
    {
      "epoch": 0.643415953791036,
      "grad_norm": 0.7137050032615662,
      "learning_rate": 0.00018714619883040937,
      "loss": 1.4071,
      "step": 2200
    },
    {
      "epoch": 0.6580390436499233,
      "grad_norm": 0.678568422794342,
      "learning_rate": 0.00018685380116959067,
      "loss": 1.4178,
      "step": 2250
    },
    {
      "epoch": 0.6726621335088104,
      "grad_norm": 0.9695717096328735,
      "learning_rate": 0.00018656140350877193,
      "loss": 1.5011,
      "step": 2300
    },
    {
      "epoch": 0.6872852233676976,
      "grad_norm": 0.49032092094421387,
      "learning_rate": 0.00018626900584795323,
      "loss": 1.4104,
      "step": 2350
    },
    {
      "epoch": 0.7019083132265848,
      "grad_norm": 0.6228231191635132,
      "learning_rate": 0.00018597660818713452,
      "loss": 1.4318,
      "step": 2400
    },
    {
      "epoch": 0.7165314030854719,
      "grad_norm": 0.5961070656776428,
      "learning_rate": 0.0001856842105263158,
      "loss": 1.4064,
      "step": 2450
    },
    {
      "epoch": 0.7311544929443592,
      "grad_norm": 0.6834394931793213,
      "learning_rate": 0.00018539181286549708,
      "loss": 1.2893,
      "step": 2500
    },
    {
      "epoch": 0.7457775828032464,
      "grad_norm": 0.43686026334762573,
      "learning_rate": 0.00018509941520467838,
      "loss": 1.3789,
      "step": 2550
    },
    {
      "epoch": 0.7604006726621335,
      "grad_norm": 0.6116247177124023,
      "learning_rate": 0.00018480701754385964,
      "loss": 1.3927,
      "step": 2600
    },
    {
      "epoch": 0.7750237625210207,
      "grad_norm": 0.8306903839111328,
      "learning_rate": 0.00018451461988304096,
      "loss": 1.3508,
      "step": 2650
    },
    {
      "epoch": 0.7896468523799078,
      "grad_norm": 0.926162600517273,
      "learning_rate": 0.00018422222222222223,
      "loss": 1.4057,
      "step": 2700
    },
    {
      "epoch": 0.8042699422387951,
      "grad_norm": 0.4762759506702423,
      "learning_rate": 0.0001839298245614035,
      "loss": 1.3934,
      "step": 2750
    },
    {
      "epoch": 0.8188930320976823,
      "grad_norm": 0.8816239237785339,
      "learning_rate": 0.00018363742690058482,
      "loss": 1.358,
      "step": 2800
    },
    {
      "epoch": 0.8335161219565694,
      "grad_norm": 0.6182754635810852,
      "learning_rate": 0.00018334502923976609,
      "loss": 1.3949,
      "step": 2850
    },
    {
      "epoch": 0.8481392118154566,
      "grad_norm": 0.6291531920433044,
      "learning_rate": 0.00018305263157894738,
      "loss": 1.458,
      "step": 2900
    },
    {
      "epoch": 0.8627623016743438,
      "grad_norm": 0.6952774524688721,
      "learning_rate": 0.00018276023391812867,
      "loss": 1.4574,
      "step": 2950
    },
    {
      "epoch": 0.8773853915332309,
      "grad_norm": 0.8317999243736267,
      "learning_rate": 0.00018246783625730994,
      "loss": 1.345,
      "step": 3000
    },
    {
      "epoch": 0.8920084813921182,
      "grad_norm": 0.7924284338951111,
      "learning_rate": 0.00018217543859649123,
      "loss": 1.402,
      "step": 3050
    },
    {
      "epoch": 0.9066315712510054,
      "grad_norm": 0.5628262758255005,
      "learning_rate": 0.00018188304093567253,
      "loss": 1.4454,
      "step": 3100
    },
    {
      "epoch": 0.9212546611098925,
      "grad_norm": 0.9600797891616821,
      "learning_rate": 0.0001815906432748538,
      "loss": 1.3824,
      "step": 3150
    },
    {
      "epoch": 0.9358777509687797,
      "grad_norm": 0.8517034649848938,
      "learning_rate": 0.0001812982456140351,
      "loss": 1.39,
      "step": 3200
    },
    {
      "epoch": 0.9505008408276668,
      "grad_norm": 0.4571148455142975,
      "learning_rate": 0.00018100584795321638,
      "loss": 1.4393,
      "step": 3250
    },
    {
      "epoch": 0.9651239306865541,
      "grad_norm": 0.5181262493133545,
      "learning_rate": 0.00018071345029239768,
      "loss": 1.4546,
      "step": 3300
    },
    {
      "epoch": 0.9797470205454413,
      "grad_norm": 0.7741950750350952,
      "learning_rate": 0.00018042105263157894,
      "loss": 1.3659,
      "step": 3350
    },
    {
      "epoch": 0.9943701104043284,
      "grad_norm": 0.4065855145454407,
      "learning_rate": 0.00018012865497076024,
      "loss": 1.4501,
      "step": 3400
    },
    {
      "epoch": 1.0087738539153324,
      "grad_norm": 0.5193621516227722,
      "learning_rate": 0.00017983625730994153,
      "loss": 1.4215,
      "step": 3450
    },
    {
      "epoch": 1.0233969437742194,
      "grad_norm": 0.8830656409263611,
      "learning_rate": 0.00017954385964912283,
      "loss": 1.2561,
      "step": 3500
    },
    {
      "epoch": 1.0380200336331067,
      "grad_norm": 0.6108176708221436,
      "learning_rate": 0.0001792514619883041,
      "loss": 1.3507,
      "step": 3550
    },
    {
      "epoch": 1.052643123491994,
      "grad_norm": 0.6351854801177979,
      "learning_rate": 0.0001789590643274854,
      "loss": 1.3346,
      "step": 3600
    },
    {
      "epoch": 1.067266213350881,
      "grad_norm": 0.6164528727531433,
      "learning_rate": 0.00017866666666666668,
      "loss": 1.4121,
      "step": 3650
    },
    {
      "epoch": 1.0818893032097683,
      "grad_norm": 1.0208030939102173,
      "learning_rate": 0.00017837426900584798,
      "loss": 1.4421,
      "step": 3700
    },
    {
      "epoch": 1.0965123930686553,
      "grad_norm": 0.49943602085113525,
      "learning_rate": 0.00017808187134502924,
      "loss": 1.3313,
      "step": 3750
    },
    {
      "epoch": 1.1111354829275426,
      "grad_norm": 0.6449347138404846,
      "learning_rate": 0.00017778947368421054,
      "loss": 1.4182,
      "step": 3800
    },
    {
      "epoch": 1.1257585727864297,
      "grad_norm": 0.5515366196632385,
      "learning_rate": 0.00017749707602339183,
      "loss": 1.4515,
      "step": 3850
    },
    {
      "epoch": 1.140381662645317,
      "grad_norm": 0.9314694404602051,
      "learning_rate": 0.0001772046783625731,
      "loss": 1.4139,
      "step": 3900
    },
    {
      "epoch": 1.1550047525042042,
      "grad_norm": 0.4439646005630493,
      "learning_rate": 0.0001769122807017544,
      "loss": 1.4111,
      "step": 3950
    },
    {
      "epoch": 1.1696278423630913,
      "grad_norm": 0.9664852619171143,
      "learning_rate": 0.00017661988304093569,
      "loss": 1.3522,
      "step": 4000
    },
    {
      "epoch": 1.1842509322219785,
      "grad_norm": 0.6828444004058838,
      "learning_rate": 0.00017632748538011695,
      "loss": 1.2907,
      "step": 4050
    },
    {
      "epoch": 1.1988740220808656,
      "grad_norm": 0.8966568112373352,
      "learning_rate": 0.00017603508771929827,
      "loss": 1.2791,
      "step": 4100
    },
    {
      "epoch": 1.2134971119397528,
      "grad_norm": 0.5963769555091858,
      "learning_rate": 0.00017574269005847954,
      "loss": 1.3801,
      "step": 4150
    },
    {
      "epoch": 1.2281202017986401,
      "grad_norm": 0.6735951900482178,
      "learning_rate": 0.0001754502923976608,
      "loss": 1.4079,
      "step": 4200
    },
    {
      "epoch": 1.2427432916575272,
      "grad_norm": 0.6047676205635071,
      "learning_rate": 0.00017515789473684213,
      "loss": 1.4196,
      "step": 4250
    },
    {
      "epoch": 1.2573663815164144,
      "grad_norm": 0.7595836520195007,
      "learning_rate": 0.0001748654970760234,
      "loss": 1.3946,
      "step": 4300
    },
    {
      "epoch": 1.2719894713753015,
      "grad_norm": 0.5106567144393921,
      "learning_rate": 0.0001745730994152047,
      "loss": 1.4138,
      "step": 4350
    },
    {
      "epoch": 1.2866125612341888,
      "grad_norm": 0.7425997853279114,
      "learning_rate": 0.00017428070175438598,
      "loss": 1.3882,
      "step": 4400
    },
    {
      "epoch": 1.301235651093076,
      "grad_norm": 0.7497231364250183,
      "learning_rate": 0.00017398830409356725,
      "loss": 1.3505,
      "step": 4450
    },
    {
      "epoch": 1.315858740951963,
      "grad_norm": 0.7428188920021057,
      "learning_rate": 0.00017369590643274854,
      "loss": 1.3926,
      "step": 4500
    },
    {
      "epoch": 1.3304818308108504,
      "grad_norm": 0.6579952836036682,
      "learning_rate": 0.00017340350877192984,
      "loss": 1.3838,
      "step": 4550
    },
    {
      "epoch": 1.3451049206697374,
      "grad_norm": 0.9336761832237244,
      "learning_rate": 0.0001731111111111111,
      "loss": 1.3956,
      "step": 4600
    },
    {
      "epoch": 1.3597280105286247,
      "grad_norm": 0.5202246904373169,
      "learning_rate": 0.00017281871345029243,
      "loss": 1.3236,
      "step": 4650
    },
    {
      "epoch": 1.374351100387512,
      "grad_norm": 0.9782015085220337,
      "learning_rate": 0.0001725263157894737,
      "loss": 1.4308,
      "step": 4700
    },
    {
      "epoch": 1.388974190246399,
      "grad_norm": 0.5955093502998352,
      "learning_rate": 0.00017223391812865496,
      "loss": 1.2931,
      "step": 4750
    },
    {
      "epoch": 1.4035972801052863,
      "grad_norm": 0.6592782139778137,
      "learning_rate": 0.00017194152046783628,
      "loss": 1.4519,
      "step": 4800
    },
    {
      "epoch": 1.4182203699641733,
      "grad_norm": 0.6282902359962463,
      "learning_rate": 0.00017164912280701755,
      "loss": 1.3739,
      "step": 4850
    },
    {
      "epoch": 1.4328434598230606,
      "grad_norm": 0.5754852294921875,
      "learning_rate": 0.00017135672514619884,
      "loss": 1.3359,
      "step": 4900
    },
    {
      "epoch": 1.4474665496819479,
      "grad_norm": 0.7941360473632812,
      "learning_rate": 0.00017106432748538014,
      "loss": 1.2688,
      "step": 4950
    },
    {
      "epoch": 1.462089639540835,
      "grad_norm": 0.7078421711921692,
      "learning_rate": 0.0001707719298245614,
      "loss": 1.3817,
      "step": 5000
    },
    {
      "epoch": 1.4767127293997222,
      "grad_norm": 0.7203848361968994,
      "learning_rate": 0.0001704795321637427,
      "loss": 1.3687,
      "step": 5050
    },
    {
      "epoch": 1.4913358192586093,
      "grad_norm": 0.5487469434738159,
      "learning_rate": 0.000170187134502924,
      "loss": 1.3202,
      "step": 5100
    },
    {
      "epoch": 1.5059589091174965,
      "grad_norm": 0.6001423001289368,
      "learning_rate": 0.00016989473684210526,
      "loss": 1.374,
      "step": 5150
    },
    {
      "epoch": 1.5205819989763838,
      "grad_norm": 0.7695392966270447,
      "learning_rate": 0.00016960233918128655,
      "loss": 1.364,
      "step": 5200
    },
    {
      "epoch": 1.5352050888352708,
      "grad_norm": 0.6862672567367554,
      "learning_rate": 0.00016930994152046785,
      "loss": 1.4399,
      "step": 5250
    },
    {
      "epoch": 1.5498281786941581,
      "grad_norm": 0.5260910391807556,
      "learning_rate": 0.00016901754385964914,
      "loss": 1.4051,
      "step": 5300
    },
    {
      "epoch": 1.5644512685530452,
      "grad_norm": 0.5614352822303772,
      "learning_rate": 0.0001687251461988304,
      "loss": 1.3518,
      "step": 5350
    },
    {
      "epoch": 1.5790743584119324,
      "grad_norm": 0.8876745700836182,
      "learning_rate": 0.0001684327485380117,
      "loss": 1.3768,
      "step": 5400
    },
    {
      "epoch": 1.5936974482708197,
      "grad_norm": 0.629252016544342,
      "learning_rate": 0.00016814619883040937,
      "loss": 1.4083,
      "step": 5450
    },
    {
      "epoch": 1.6083205381297068,
      "grad_norm": 0.7718436121940613,
      "learning_rate": 0.00016785380116959064,
      "loss": 1.4019,
      "step": 5500
    },
    {
      "epoch": 1.622943627988594,
      "grad_norm": 0.4306371212005615,
      "learning_rate": 0.00016756140350877193,
      "loss": 1.3736,
      "step": 5550
    },
    {
      "epoch": 1.637566717847481,
      "grad_norm": 0.8484540581703186,
      "learning_rate": 0.00016726900584795323,
      "loss": 1.3274,
      "step": 5600
    },
    {
      "epoch": 1.6521898077063684,
      "grad_norm": 0.6003826260566711,
      "learning_rate": 0.00016697660818713452,
      "loss": 1.3449,
      "step": 5650
    },
    {
      "epoch": 1.6668128975652556,
      "grad_norm": 0.8320145010948181,
      "learning_rate": 0.0001666842105263158,
      "loss": 1.3992,
      "step": 5700
    },
    {
      "epoch": 1.6814359874241427,
      "grad_norm": 0.42535948753356934,
      "learning_rate": 0.00016639181286549708,
      "loss": 1.3461,
      "step": 5750
    },
    {
      "epoch": 1.69605907728303,
      "grad_norm": 1.041745662689209,
      "learning_rate": 0.00016609941520467837,
      "loss": 1.4042,
      "step": 5800
    },
    {
      "epoch": 1.710682167141917,
      "grad_norm": 0.5812271237373352,
      "learning_rate": 0.00016580701754385964,
      "loss": 1.4139,
      "step": 5850
    },
    {
      "epoch": 1.7253052570008043,
      "grad_norm": 0.7751721739768982,
      "learning_rate": 0.00016551461988304094,
      "loss": 1.3489,
      "step": 5900
    },
    {
      "epoch": 1.7399283468596916,
      "grad_norm": 0.7072997093200684,
      "learning_rate": 0.00016522222222222223,
      "loss": 1.3541,
      "step": 5950
    },
    {
      "epoch": 1.7545514367185786,
      "grad_norm": 0.6442375183105469,
      "learning_rate": 0.00016492982456140352,
      "loss": 1.3548,
      "step": 6000
    },
    {
      "epoch": 1.7691745265774657,
      "grad_norm": 1.1720205545425415,
      "learning_rate": 0.00016463742690058482,
      "loss": 1.403,
      "step": 6050
    },
    {
      "epoch": 1.783797616436353,
      "grad_norm": 0.5215203762054443,
      "learning_rate": 0.00016434502923976608,
      "loss": 1.3565,
      "step": 6100
    },
    {
      "epoch": 1.7984207062952402,
      "grad_norm": 0.7793706655502319,
      "learning_rate": 0.00016405263157894738,
      "loss": 1.3562,
      "step": 6150
    },
    {
      "epoch": 1.8130437961541275,
      "grad_norm": 0.7570998668670654,
      "learning_rate": 0.00016376023391812867,
      "loss": 1.3847,
      "step": 6200
    },
    {
      "epoch": 1.8276668860130145,
      "grad_norm": 0.4798611104488373,
      "learning_rate": 0.00016346783625730994,
      "loss": 1.329,
      "step": 6250
    },
    {
      "epoch": 1.8422899758719016,
      "grad_norm": 0.5399637222290039,
      "learning_rate": 0.00016317543859649123,
      "loss": 1.3897,
      "step": 6300
    },
    {
      "epoch": 1.8569130657307888,
      "grad_norm": 0.48709896206855774,
      "learning_rate": 0.00016288304093567253,
      "loss": 1.3372,
      "step": 6350
    },
    {
      "epoch": 1.8715361555896761,
      "grad_norm": 0.6058634519577026,
      "learning_rate": 0.0001625906432748538,
      "loss": 1.4536,
      "step": 6400
    },
    {
      "epoch": 1.8861592454485634,
      "grad_norm": 0.5530894994735718,
      "learning_rate": 0.00016229824561403512,
      "loss": 1.4074,
      "step": 6450
    },
    {
      "epoch": 1.9007823353074504,
      "grad_norm": 0.8902642726898193,
      "learning_rate": 0.00016200584795321638,
      "loss": 1.3388,
      "step": 6500
    },
    {
      "epoch": 1.9154054251663375,
      "grad_norm": 0.62203049659729,
      "learning_rate": 0.00016171345029239765,
      "loss": 1.3363,
      "step": 6550
    },
    {
      "epoch": 1.9300285150252248,
      "grad_norm": 0.4428621232509613,
      "learning_rate": 0.00016142105263157897,
      "loss": 1.375,
      "step": 6600
    },
    {
      "epoch": 1.944651604884112,
      "grad_norm": 0.7121002674102783,
      "learning_rate": 0.00016112865497076024,
      "loss": 1.3484,
      "step": 6650
    },
    {
      "epoch": 1.9592746947429993,
      "grad_norm": 0.8395596742630005,
      "learning_rate": 0.00016083625730994153,
      "loss": 1.2732,
      "step": 6700
    },
    {
      "epoch": 1.9738977846018864,
      "grad_norm": 0.9725739359855652,
      "learning_rate": 0.00016054385964912283,
      "loss": 1.3953,
      "step": 6750
    },
    {
      "epoch": 1.9885208744607734,
      "grad_norm": 0.754680871963501,
      "learning_rate": 0.0001602514619883041,
      "loss": 1.3767,
      "step": 6800
    },
    {
      "epoch": 2.0029246179717775,
      "grad_norm": 0.47889411449432373,
      "learning_rate": 0.0001599590643274854,
      "loss": 1.3148,
      "step": 6850
    },
    {
      "epoch": 2.0175477078306647,
      "grad_norm": 0.6854554414749146,
      "learning_rate": 0.00015966666666666668,
      "loss": 1.3105,
      "step": 6900
    },
    {
      "epoch": 2.032170797689552,
      "grad_norm": 0.8067441582679749,
      "learning_rate": 0.00015937426900584795,
      "loss": 1.3484,
      "step": 6950
    },
    {
      "epoch": 2.046793887548439,
      "grad_norm": 0.7420204281806946,
      "learning_rate": 0.00015908187134502924,
      "loss": 1.3126,
      "step": 7000
    },
    {
      "epoch": 2.061416977407326,
      "grad_norm": 0.49466171860694885,
      "learning_rate": 0.00015878947368421054,
      "loss": 1.3941,
      "step": 7050
    },
    {
      "epoch": 2.0760400672662134,
      "grad_norm": 0.8745059370994568,
      "learning_rate": 0.0001584970760233918,
      "loss": 1.3253,
      "step": 7100
    },
    {
      "epoch": 2.0906631571251006,
      "grad_norm": 0.7573168873786926,
      "learning_rate": 0.00015820467836257312,
      "loss": 1.3312,
      "step": 7150
    },
    {
      "epoch": 2.105286246983988,
      "grad_norm": 1.0312479734420776,
      "learning_rate": 0.0001579122807017544,
      "loss": 1.2902,
      "step": 7200
    },
    {
      "epoch": 2.1199093368428747,
      "grad_norm": 0.7872934341430664,
      "learning_rate": 0.00015761988304093568,
      "loss": 1.3429,
      "step": 7250
    },
    {
      "epoch": 2.134532426701762,
      "grad_norm": 0.7727640271186829,
      "learning_rate": 0.00015732748538011698,
      "loss": 1.3002,
      "step": 7300
    },
    {
      "epoch": 2.1491555165606493,
      "grad_norm": 0.7001848816871643,
      "learning_rate": 0.00015703508771929825,
      "loss": 1.3667,
      "step": 7350
    },
    {
      "epoch": 2.1637786064195366,
      "grad_norm": 0.8987118005752563,
      "learning_rate": 0.00015674269005847954,
      "loss": 1.3642,
      "step": 7400
    },
    {
      "epoch": 2.178401696278424,
      "grad_norm": 1.0879958868026733,
      "learning_rate": 0.00015645029239766083,
      "loss": 1.3421,
      "step": 7450
    },
    {
      "epoch": 2.1930247861373107,
      "grad_norm": 0.6579390168190002,
      "learning_rate": 0.0001561578947368421,
      "loss": 1.3195,
      "step": 7500
    },
    {
      "epoch": 2.207647875996198,
      "grad_norm": 0.6283301115036011,
      "learning_rate": 0.0001558654970760234,
      "loss": 1.2706,
      "step": 7550
    },
    {
      "epoch": 2.222270965855085,
      "grad_norm": 0.6848675012588501,
      "learning_rate": 0.0001555730994152047,
      "loss": 1.4004,
      "step": 7600
    },
    {
      "epoch": 2.2368940557139725,
      "grad_norm": 0.5734383463859558,
      "learning_rate": 0.00015528070175438598,
      "loss": 1.2837,
      "step": 7650
    },
    {
      "epoch": 2.2515171455728593,
      "grad_norm": 0.7939313054084778,
      "learning_rate": 0.00015498830409356725,
      "loss": 1.3291,
      "step": 7700
    },
    {
      "epoch": 2.2661402354317466,
      "grad_norm": 0.9449202418327332,
      "learning_rate": 0.00015469590643274854,
      "loss": 1.3502,
      "step": 7750
    },
    {
      "epoch": 2.280763325290634,
      "grad_norm": 1.0456794500350952,
      "learning_rate": 0.00015440350877192984,
      "loss": 1.3378,
      "step": 7800
    },
    {
      "epoch": 2.295386415149521,
      "grad_norm": 0.5201883912086487,
      "learning_rate": 0.0001541111111111111,
      "loss": 1.3147,
      "step": 7850
    },
    {
      "epoch": 2.3100095050084084,
      "grad_norm": 0.8193795680999756,
      "learning_rate": 0.0001538187134502924,
      "loss": 1.3628,
      "step": 7900
    },
    {
      "epoch": 2.3246325948672952,
      "grad_norm": 0.8234537839889526,
      "learning_rate": 0.0001535263157894737,
      "loss": 1.3873,
      "step": 7950
    },
    {
      "epoch": 2.3392556847261825,
      "grad_norm": 1.1135473251342773,
      "learning_rate": 0.000153233918128655,
      "loss": 1.2955,
      "step": 8000
    },
    {
      "epoch": 2.3538787745850698,
      "grad_norm": 1.0240381956100464,
      "learning_rate": 0.00015294152046783628,
      "loss": 1.3265,
      "step": 8050
    },
    {
      "epoch": 2.368501864443957,
      "grad_norm": 0.7970229387283325,
      "learning_rate": 0.00015264912280701755,
      "loss": 1.3377,
      "step": 8100
    },
    {
      "epoch": 2.3831249543028443,
      "grad_norm": 0.9090719819068909,
      "learning_rate": 0.00015235672514619884,
      "loss": 1.3746,
      "step": 8150
    },
    {
      "epoch": 2.397748044161731,
      "grad_norm": 0.7834468483924866,
      "learning_rate": 0.00015206432748538014,
      "loss": 1.3986,
      "step": 8200
    },
    {
      "epoch": 2.4123711340206184,
      "grad_norm": 1.1005347967147827,
      "learning_rate": 0.0001517719298245614,
      "loss": 1.3359,
      "step": 8250
    },
    {
      "epoch": 2.4269942238795057,
      "grad_norm": 1.0556436777114868,
      "learning_rate": 0.0001514795321637427,
      "loss": 1.3484,
      "step": 8300
    },
    {
      "epoch": 2.441617313738393,
      "grad_norm": 0.9422598481178284,
      "learning_rate": 0.000151187134502924,
      "loss": 1.4077,
      "step": 8350
    },
    {
      "epoch": 2.4562404035972802,
      "grad_norm": 0.7032132744789124,
      "learning_rate": 0.00015089473684210526,
      "loss": 1.3589,
      "step": 8400
    },
    {
      "epoch": 2.470863493456167,
      "grad_norm": 0.9831897616386414,
      "learning_rate": 0.00015060233918128658,
      "loss": 1.4172,
      "step": 8450
    },
    {
      "epoch": 2.4854865833150543,
      "grad_norm": 0.7805212736129761,
      "learning_rate": 0.00015030994152046785,
      "loss": 1.3722,
      "step": 8500
    },
    {
      "epoch": 2.5001096731739416,
      "grad_norm": 0.6808881163597107,
      "learning_rate": 0.0001500175438596491,
      "loss": 1.3654,
      "step": 8550
    },
    {
      "epoch": 2.514732763032829,
      "grad_norm": 0.7664763927459717,
      "learning_rate": 0.00014972514619883043,
      "loss": 1.3491,
      "step": 8600
    },
    {
      "epoch": 2.529355852891716,
      "grad_norm": 0.6929658651351929,
      "learning_rate": 0.0001494327485380117,
      "loss": 1.3262,
      "step": 8650
    },
    {
      "epoch": 2.543978942750603,
      "grad_norm": 0.8613722920417786,
      "learning_rate": 0.00014914035087719297,
      "loss": 1.3612,
      "step": 8700
    },
    {
      "epoch": 2.5586020326094903,
      "grad_norm": 0.6739090085029602,
      "learning_rate": 0.0001488479532163743,
      "loss": 1.3754,
      "step": 8750
    },
    {
      "epoch": 2.5732251224683775,
      "grad_norm": 0.8733100295066833,
      "learning_rate": 0.00014855555555555556,
      "loss": 1.287,
      "step": 8800
    },
    {
      "epoch": 2.587848212327265,
      "grad_norm": 0.7268351912498474,
      "learning_rate": 0.00014826315789473685,
      "loss": 1.3919,
      "step": 8850
    },
    {
      "epoch": 2.602471302186152,
      "grad_norm": 0.8491650819778442,
      "learning_rate": 0.00014797076023391814,
      "loss": 1.3684,
      "step": 8900
    },
    {
      "epoch": 2.617094392045039,
      "grad_norm": 0.6410504579544067,
      "learning_rate": 0.0001476783625730994,
      "loss": 1.4067,
      "step": 8950
    },
    {
      "epoch": 2.631717481903926,
      "grad_norm": 0.70405113697052,
      "learning_rate": 0.0001473859649122807,
      "loss": 1.3275,
      "step": 9000
    },
    {
      "epoch": 2.6463405717628135,
      "grad_norm": 0.7070156931877136,
      "learning_rate": 0.000147093567251462,
      "loss": 1.3263,
      "step": 9050
    },
    {
      "epoch": 2.6609636616217007,
      "grad_norm": 1.0053913593292236,
      "learning_rate": 0.00014680116959064327,
      "loss": 1.3781,
      "step": 9100
    },
    {
      "epoch": 2.675586751480588,
      "grad_norm": 0.6471641659736633,
      "learning_rate": 0.0001465087719298246,
      "loss": 1.3078,
      "step": 9150
    },
    {
      "epoch": 2.690209841339475,
      "grad_norm": 0.7681560516357422,
      "learning_rate": 0.00014621637426900585,
      "loss": 1.3218,
      "step": 9200
    },
    {
      "epoch": 2.704832931198362,
      "grad_norm": 1.1317274570465088,
      "learning_rate": 0.00014592397660818715,
      "loss": 1.404,
      "step": 9250
    },
    {
      "epoch": 2.7194560210572494,
      "grad_norm": 0.7695015668869019,
      "learning_rate": 0.00014563157894736844,
      "loss": 1.3651,
      "step": 9300
    },
    {
      "epoch": 2.7340791109161366,
      "grad_norm": 0.7324898838996887,
      "learning_rate": 0.0001453391812865497,
      "loss": 1.3037,
      "step": 9350
    },
    {
      "epoch": 2.748702200775024,
      "grad_norm": 0.8355674147605896,
      "learning_rate": 0.000145046783625731,
      "loss": 1.3236,
      "step": 9400
    },
    {
      "epoch": 2.7633252906339107,
      "grad_norm": 0.6624045968055725,
      "learning_rate": 0.0001447543859649123,
      "loss": 1.3967,
      "step": 9450
    },
    {
      "epoch": 2.777948380492798,
      "grad_norm": 0.9829651117324829,
      "learning_rate": 0.00014446783625730994,
      "loss": 1.3403,
      "step": 9500
    },
    {
      "epoch": 2.7925714703516853,
      "grad_norm": 0.813948929309845,
      "learning_rate": 0.00014417543859649123,
      "loss": 1.3774,
      "step": 9550
    },
    {
      "epoch": 2.8071945602105726,
      "grad_norm": 0.3989418148994446,
      "learning_rate": 0.00014388304093567253,
      "loss": 1.4114,
      "step": 9600
    },
    {
      "epoch": 2.82181765006946,
      "grad_norm": 1.0039540529251099,
      "learning_rate": 0.00014359064327485382,
      "loss": 1.4166,
      "step": 9650
    },
    {
      "epoch": 2.8364407399283467,
      "grad_norm": 0.9892141819000244,
      "learning_rate": 0.00014330409356725146,
      "loss": 1.2972,
      "step": 9700
    },
    {
      "epoch": 2.851063829787234,
      "grad_norm": 0.6896465420722961,
      "learning_rate": 0.00014301169590643276,
      "loss": 1.3599,
      "step": 9750
    },
    {
      "epoch": 2.865686919646121,
      "grad_norm": 0.875856339931488,
      "learning_rate": 0.00014271929824561405,
      "loss": 1.3483,
      "step": 9800
    },
    {
      "epoch": 2.8803100095050085,
      "grad_norm": 0.8820026516914368,
      "learning_rate": 0.00014242690058479532,
      "loss": 1.4113,
      "step": 9850
    },
    {
      "epoch": 2.8949330993638958,
      "grad_norm": 0.7437812685966492,
      "learning_rate": 0.0001421345029239766,
      "loss": 1.3446,
      "step": 9900
    },
    {
      "epoch": 2.9095561892227826,
      "grad_norm": 0.6578355431556702,
      "learning_rate": 0.0001418421052631579,
      "loss": 1.3046,
      "step": 9950
    },
    {
      "epoch": 2.92417927908167,
      "grad_norm": 0.8326758742332458,
      "learning_rate": 0.0001415497076023392,
      "loss": 1.257,
      "step": 10000
    },
    {
      "epoch": 2.938802368940557,
      "grad_norm": 0.7699238657951355,
      "learning_rate": 0.00014125730994152047,
      "loss": 1.3175,
      "step": 10050
    },
    {
      "epoch": 2.9534254587994444,
      "grad_norm": 1.025371789932251,
      "learning_rate": 0.00014096491228070176,
      "loss": 1.3688,
      "step": 10100
    },
    {
      "epoch": 2.9680485486583317,
      "grad_norm": 0.6839094758033752,
      "learning_rate": 0.00014067251461988305,
      "loss": 1.355,
      "step": 10150
    },
    {
      "epoch": 2.9826716385172185,
      "grad_norm": 1.0017294883728027,
      "learning_rate": 0.00014038011695906435,
      "loss": 1.3256,
      "step": 10200
    },
    {
      "epoch": 2.9972947283761058,
      "grad_norm": 0.5257346630096436,
      "learning_rate": 0.00014008771929824562,
      "loss": 1.41,
      "step": 10250
    },
    {
      "epoch": 3.01169847188711,
      "grad_norm": 0.6972954869270325,
      "learning_rate": 0.0001397953216374269,
      "loss": 1.2154,
      "step": 10300
    },
    {
      "epoch": 3.026321561745997,
      "grad_norm": 0.8400363922119141,
      "learning_rate": 0.0001395029239766082,
      "loss": 1.3236,
      "step": 10350
    },
    {
      "epoch": 3.040944651604884,
      "grad_norm": 0.9254976511001587,
      "learning_rate": 0.00013921052631578947,
      "loss": 1.3521,
      "step": 10400
    },
    {
      "epoch": 3.055567741463771,
      "grad_norm": 0.6778669953346252,
      "learning_rate": 0.00013891812865497076,
      "loss": 1.3432,
      "step": 10450
    },
    {
      "epoch": 3.0701908313226585,
      "grad_norm": 0.894140362739563,
      "learning_rate": 0.00013862573099415206,
      "loss": 1.3392,
      "step": 10500
    },
    {
      "epoch": 3.0848139211815457,
      "grad_norm": 0.8205103278160095,
      "learning_rate": 0.00013833333333333333,
      "loss": 1.3251,
      "step": 10550
    },
    {
      "epoch": 3.099437011040433,
      "grad_norm": 0.640436589717865,
      "learning_rate": 0.00013804093567251465,
      "loss": 1.2686,
      "step": 10600
    },
    {
      "epoch": 3.11406010089932,
      "grad_norm": 0.702284038066864,
      "learning_rate": 0.00013774853801169591,
      "loss": 1.3826,
      "step": 10650
    },
    {
      "epoch": 3.128683190758207,
      "grad_norm": 0.8930631279945374,
      "learning_rate": 0.00013745614035087718,
      "loss": 1.3456,
      "step": 10700
    },
    {
      "epoch": 3.1433062806170944,
      "grad_norm": 1.0019744634628296,
      "learning_rate": 0.0001371637426900585,
      "loss": 1.3371,
      "step": 10750
    },
    {
      "epoch": 3.1579293704759817,
      "grad_norm": 0.8716136813163757,
      "learning_rate": 0.00013687134502923977,
      "loss": 1.3437,
      "step": 10800
    },
    {
      "epoch": 3.172552460334869,
      "grad_norm": 0.7653725147247314,
      "learning_rate": 0.00013657894736842106,
      "loss": 1.2652,
      "step": 10850
    },
    {
      "epoch": 3.1871755501937558,
      "grad_norm": 0.8758544921875,
      "learning_rate": 0.00013628654970760236,
      "loss": 1.3353,
      "step": 10900
    },
    {
      "epoch": 3.201798640052643,
      "grad_norm": 0.959363579750061,
      "learning_rate": 0.00013599415204678362,
      "loss": 1.3127,
      "step": 10950
    },
    {
      "epoch": 3.2164217299115303,
      "grad_norm": 1.1327049732208252,
      "learning_rate": 0.00013570175438596492,
      "loss": 1.2762,
      "step": 11000
    },
    {
      "epoch": 3.2310448197704176,
      "grad_norm": 1.007926344871521,
      "learning_rate": 0.0001354093567251462,
      "loss": 1.312,
      "step": 11050
    },
    {
      "epoch": 3.245667909629305,
      "grad_norm": 0.8145801424980164,
      "learning_rate": 0.00013511695906432748,
      "loss": 1.3246,
      "step": 11100
    },
    {
      "epoch": 3.2602909994881917,
      "grad_norm": 0.7350960969924927,
      "learning_rate": 0.0001348245614035088,
      "loss": 1.3086,
      "step": 11150
    },
    {
      "epoch": 3.274914089347079,
      "grad_norm": 1.0635826587677002,
      "learning_rate": 0.00013453216374269007,
      "loss": 1.2636,
      "step": 11200
    },
    {
      "epoch": 3.289537179205966,
      "grad_norm": 0.6905484795570374,
      "learning_rate": 0.00013423976608187133,
      "loss": 1.354,
      "step": 11250
    },
    {
      "epoch": 3.3041602690648535,
      "grad_norm": 1.2448550462722778,
      "learning_rate": 0.00013394736842105265,
      "loss": 1.3396,
      "step": 11300
    },
    {
      "epoch": 3.3187833589237408,
      "grad_norm": 0.9882857203483582,
      "learning_rate": 0.00013365497076023392,
      "loss": 1.3104,
      "step": 11350
    },
    {
      "epoch": 3.3334064487826276,
      "grad_norm": 0.8020055294036865,
      "learning_rate": 0.0001333625730994152,
      "loss": 1.2716,
      "step": 11400
    },
    {
      "epoch": 3.348029538641515,
      "grad_norm": 0.6877641677856445,
      "learning_rate": 0.0001330701754385965,
      "loss": 1.2967,
      "step": 11450
    },
    {
      "epoch": 3.362652628500402,
      "grad_norm": 0.9868064522743225,
      "learning_rate": 0.00013277777777777778,
      "loss": 1.2747,
      "step": 11500
    },
    {
      "epoch": 3.3772757183592894,
      "grad_norm": 0.9368124008178711,
      "learning_rate": 0.00013248538011695907,
      "loss": 1.3225,
      "step": 11550
    },
    {
      "epoch": 3.3918988082181767,
      "grad_norm": 0.8008185625076294,
      "learning_rate": 0.00013219298245614036,
      "loss": 1.2829,
      "step": 11600
    },
    {
      "epoch": 3.4065218980770635,
      "grad_norm": 1.3492093086242676,
      "learning_rate": 0.00013190058479532163,
      "loss": 1.3014,
      "step": 11650
    },
    {
      "epoch": 3.421144987935951,
      "grad_norm": 0.8469408750534058,
      "learning_rate": 0.00013160818713450293,
      "loss": 1.2913,
      "step": 11700
    },
    {
      "epoch": 3.435768077794838,
      "grad_norm": 0.931760847568512,
      "learning_rate": 0.00013131578947368422,
      "loss": 1.4261,
      "step": 11750
    },
    {
      "epoch": 3.4503911676537253,
      "grad_norm": 0.768846333026886,
      "learning_rate": 0.0001310233918128655,
      "loss": 1.2877,
      "step": 11800
    },
    {
      "epoch": 3.4650142575126126,
      "grad_norm": 0.7769486308097839,
      "learning_rate": 0.00013073099415204678,
      "loss": 1.3388,
      "step": 11850
    },
    {
      "epoch": 3.4796373473714994,
      "grad_norm": 0.6729854345321655,
      "learning_rate": 0.00013043859649122807,
      "loss": 1.314,
      "step": 11900
    },
    {
      "epoch": 3.4942604372303867,
      "grad_norm": 0.9765770435333252,
      "learning_rate": 0.00013014619883040937,
      "loss": 1.3737,
      "step": 11950
    },
    {
      "epoch": 3.508883527089274,
      "grad_norm": 1.0556199550628662,
      "learning_rate": 0.00012985380116959066,
      "loss": 1.2975,
      "step": 12000
    },
    {
      "epoch": 3.5235066169481613,
      "grad_norm": 0.9409111738204956,
      "learning_rate": 0.00012956140350877193,
      "loss": 1.338,
      "step": 12050
    },
    {
      "epoch": 3.538129706807048,
      "grad_norm": 0.7761494517326355,
      "learning_rate": 0.00012926900584795322,
      "loss": 1.3856,
      "step": 12100
    },
    {
      "epoch": 3.5527527966659354,
      "grad_norm": 0.6099523305892944,
      "learning_rate": 0.00012897660818713452,
      "loss": 1.2985,
      "step": 12150
    },
    {
      "epoch": 3.5673758865248226,
      "grad_norm": 0.7294567227363586,
      "learning_rate": 0.00012868421052631578,
      "loss": 1.3219,
      "step": 12200
    },
    {
      "epoch": 3.58199897638371,
      "grad_norm": 0.8496514558792114,
      "learning_rate": 0.00012839181286549708,
      "loss": 1.3723,
      "step": 12250
    },
    {
      "epoch": 3.596622066242597,
      "grad_norm": 0.783238410949707,
      "learning_rate": 0.00012809941520467837,
      "loss": 1.2678,
      "step": 12300
    },
    {
      "epoch": 3.611245156101484,
      "grad_norm": 0.8493760824203491,
      "learning_rate": 0.00012780701754385967,
      "loss": 1.3087,
      "step": 12350
    },
    {
      "epoch": 3.6258682459603713,
      "grad_norm": 0.8977285623550415,
      "learning_rate": 0.00012751461988304093,
      "loss": 1.3445,
      "step": 12400
    },
    {
      "epoch": 3.6404913358192585,
      "grad_norm": 0.8377053737640381,
      "learning_rate": 0.00012722222222222223,
      "loss": 1.4024,
      "step": 12450
    },
    {
      "epoch": 3.655114425678146,
      "grad_norm": 0.7530326843261719,
      "learning_rate": 0.00012692982456140352,
      "loss": 1.3506,
      "step": 12500
    },
    {
      "epoch": 3.669737515537033,
      "grad_norm": 0.7639336585998535,
      "learning_rate": 0.0001266374269005848,
      "loss": 1.2949,
      "step": 12550
    },
    {
      "epoch": 3.68436060539592,
      "grad_norm": 0.7692288756370544,
      "learning_rate": 0.00012634502923976608,
      "loss": 1.2985,
      "step": 12600
    },
    {
      "epoch": 3.698983695254807,
      "grad_norm": 0.7401300072669983,
      "learning_rate": 0.00012605263157894738,
      "loss": 1.3423,
      "step": 12650
    },
    {
      "epoch": 3.7136067851136945,
      "grad_norm": 0.7403785586357117,
      "learning_rate": 0.00012576023391812864,
      "loss": 1.2506,
      "step": 12700
    },
    {
      "epoch": 3.7282298749725817,
      "grad_norm": 0.6325333714485168,
      "learning_rate": 0.00012546783625730996,
      "loss": 1.3407,
      "step": 12750
    },
    {
      "epoch": 3.742852964831469,
      "grad_norm": 0.764335036277771,
      "learning_rate": 0.0001251812865497076,
      "loss": 1.3008,
      "step": 12800
    },
    {
      "epoch": 3.757476054690356,
      "grad_norm": 0.6223526000976562,
      "learning_rate": 0.0001248888888888889,
      "loss": 1.2609,
      "step": 12850
    },
    {
      "epoch": 3.772099144549243,
      "grad_norm": 0.8970713019371033,
      "learning_rate": 0.00012459649122807017,
      "loss": 1.3569,
      "step": 12900
    },
    {
      "epoch": 3.7867222344081304,
      "grad_norm": 0.851660966873169,
      "learning_rate": 0.0001243040935672515,
      "loss": 1.3725,
      "step": 12950
    },
    {
      "epoch": 3.8013453242670177,
      "grad_norm": 1.1122055053710938,
      "learning_rate": 0.00012401169590643276,
      "loss": 1.3149,
      "step": 13000
    },
    {
      "epoch": 3.815968414125905,
      "grad_norm": 0.5770636796951294,
      "learning_rate": 0.00012371929824561402,
      "loss": 1.3108,
      "step": 13050
    },
    {
      "epoch": 3.8305915039847918,
      "grad_norm": 0.9798200130462646,
      "learning_rate": 0.00012342690058479534,
      "loss": 1.4239,
      "step": 13100
    },
    {
      "epoch": 3.845214593843679,
      "grad_norm": 0.9116684794425964,
      "learning_rate": 0.0001231345029239766,
      "loss": 1.3329,
      "step": 13150
    },
    {
      "epoch": 3.8598376837025663,
      "grad_norm": 0.7532599568367004,
      "learning_rate": 0.0001228421052631579,
      "loss": 1.3212,
      "step": 13200
    },
    {
      "epoch": 3.8744607735614536,
      "grad_norm": 1.0239899158477783,
      "learning_rate": 0.0001225497076023392,
      "loss": 1.3385,
      "step": 13250
    },
    {
      "epoch": 3.889083863420341,
      "grad_norm": 1.0034548044204712,
      "learning_rate": 0.00012225730994152047,
      "loss": 1.3105,
      "step": 13300
    },
    {
      "epoch": 3.9037069532792277,
      "grad_norm": 1.2859914302825928,
      "learning_rate": 0.00012196491228070177,
      "loss": 1.3541,
      "step": 13350
    },
    {
      "epoch": 3.918330043138115,
      "grad_norm": 1.069088101387024,
      "learning_rate": 0.00012167251461988305,
      "loss": 1.2876,
      "step": 13400
    },
    {
      "epoch": 3.932953132997002,
      "grad_norm": 0.7486600875854492,
      "learning_rate": 0.00012138011695906432,
      "loss": 1.3574,
      "step": 13450
    },
    {
      "epoch": 3.9475762228558895,
      "grad_norm": 0.9864031076431274,
      "learning_rate": 0.00012108771929824563,
      "loss": 1.3425,
      "step": 13500
    },
    {
      "epoch": 3.9621993127147768,
      "grad_norm": 1.028411626815796,
      "learning_rate": 0.00012079532163742691,
      "loss": 1.2808,
      "step": 13550
    },
    {
      "epoch": 3.9768224025736636,
      "grad_norm": 1.4008005857467651,
      "learning_rate": 0.00012050292397660819,
      "loss": 1.3625,
      "step": 13600
    },
    {
      "epoch": 3.991445492432551,
      "grad_norm": 1.2062468528747559,
      "learning_rate": 0.00012021052631578948,
      "loss": 1.3548,
      "step": 13650
    },
    {
      "epoch": 4.005849235943555,
      "grad_norm": 0.8875688910484314,
      "learning_rate": 0.00011991812865497076,
      "loss": 1.3281,
      "step": 13700
    },
    {
      "epoch": 4.020472325802442,
      "grad_norm": 1.2960925102233887,
      "learning_rate": 0.00011962573099415206,
      "loss": 1.297,
      "step": 13750
    },
    {
      "epoch": 4.0350954156613295,
      "grad_norm": 0.8759080767631531,
      "learning_rate": 0.00011933333333333334,
      "loss": 1.3226,
      "step": 13800
    },
    {
      "epoch": 4.049718505520216,
      "grad_norm": 0.9328498840332031,
      "learning_rate": 0.00011904093567251462,
      "loss": 1.3075,
      "step": 13850
    },
    {
      "epoch": 4.064341595379104,
      "grad_norm": 0.6685677170753479,
      "learning_rate": 0.00011874853801169593,
      "loss": 1.3264,
      "step": 13900
    },
    {
      "epoch": 4.078964685237991,
      "grad_norm": 0.9719283580780029,
      "learning_rate": 0.00011845614035087719,
      "loss": 1.2809,
      "step": 13950
    },
    {
      "epoch": 4.093587775096878,
      "grad_norm": 0.8911837935447693,
      "learning_rate": 0.00011816374269005847,
      "loss": 1.3125,
      "step": 14000
    },
    {
      "epoch": 4.108210864955765,
      "grad_norm": 0.8941910266876221,
      "learning_rate": 0.00011787134502923978,
      "loss": 1.309,
      "step": 14050
    },
    {
      "epoch": 4.122833954814652,
      "grad_norm": 1.0863885879516602,
      "learning_rate": 0.00011757894736842106,
      "loss": 1.335,
      "step": 14100
    },
    {
      "epoch": 4.13745704467354,
      "grad_norm": 0.927891194820404,
      "learning_rate": 0.00011728654970760236,
      "loss": 1.2938,
      "step": 14150
    },
    {
      "epoch": 4.152080134532427,
      "grad_norm": 0.969723105430603,
      "learning_rate": 0.00011699415204678364,
      "loss": 1.2759,
      "step": 14200
    },
    {
      "epoch": 4.166703224391314,
      "grad_norm": 1.2142971754074097,
      "learning_rate": 0.00011670175438596492,
      "loss": 1.3417,
      "step": 14250
    },
    {
      "epoch": 4.181326314250201,
      "grad_norm": 0.8807597160339355,
      "learning_rate": 0.00011640935672514621,
      "loss": 1.3397,
      "step": 14300
    },
    {
      "epoch": 4.195949404109088,
      "grad_norm": 1.1670488119125366,
      "learning_rate": 0.00011611695906432749,
      "loss": 1.3391,
      "step": 14350
    },
    {
      "epoch": 4.210572493967976,
      "grad_norm": 0.7265651822090149,
      "learning_rate": 0.00011582456140350877,
      "loss": 1.2464,
      "step": 14400
    },
    {
      "epoch": 4.225195583826863,
      "grad_norm": 0.8468398451805115,
      "learning_rate": 0.00011553801169590643,
      "loss": 1.2432,
      "step": 14450
    },
    {
      "epoch": 4.2398186736857495,
      "grad_norm": 0.8859622478485107,
      "learning_rate": 0.00011524561403508773,
      "loss": 1.2646,
      "step": 14500
    },
    {
      "epoch": 4.254441763544637,
      "grad_norm": 0.6231204271316528,
      "learning_rate": 0.00011495321637426902,
      "loss": 1.3078,
      "step": 14550
    },
    {
      "epoch": 4.269064853403524,
      "grad_norm": 1.0476040840148926,
      "learning_rate": 0.0001146608187134503,
      "loss": 1.299,
      "step": 14600
    },
    {
      "epoch": 4.283687943262412,
      "grad_norm": 0.8506013751029968,
      "learning_rate": 0.00011436842105263159,
      "loss": 1.3087,
      "step": 14650
    },
    {
      "epoch": 4.298311033121299,
      "grad_norm": 0.9174983501434326,
      "learning_rate": 0.00011407602339181287,
      "loss": 1.2702,
      "step": 14700
    },
    {
      "epoch": 4.312934122980185,
      "grad_norm": 0.9258683323860168,
      "learning_rate": 0.00011378362573099415,
      "loss": 1.2441,
      "step": 14750
    },
    {
      "epoch": 4.327557212839073,
      "grad_norm": 1.1738508939743042,
      "learning_rate": 0.00011349122807017544,
      "loss": 1.3455,
      "step": 14800
    },
    {
      "epoch": 4.34218030269796,
      "grad_norm": 0.8668467998504639,
      "learning_rate": 0.00011319883040935673,
      "loss": 1.3222,
      "step": 14850
    },
    {
      "epoch": 4.356803392556848,
      "grad_norm": 0.8867456316947937,
      "learning_rate": 0.00011290643274853803,
      "loss": 1.2638,
      "step": 14900
    },
    {
      "epoch": 4.3714264824157345,
      "grad_norm": 1.2363042831420898,
      "learning_rate": 0.0001126140350877193,
      "loss": 1.2799,
      "step": 14950
    },
    {
      "epoch": 4.386049572274621,
      "grad_norm": 0.9397445321083069,
      "learning_rate": 0.00011232163742690058,
      "loss": 1.2444,
      "step": 15000
    },
    {
      "epoch": 4.400672662133509,
      "grad_norm": 0.8138933777809143,
      "learning_rate": 0.00011202923976608189,
      "loss": 1.3352,
      "step": 15050
    },
    {
      "epoch": 4.415295751992396,
      "grad_norm": 1.0172035694122314,
      "learning_rate": 0.00011173684210526317,
      "loss": 1.3515,
      "step": 15100
    },
    {
      "epoch": 4.429918841851283,
      "grad_norm": 0.6901199817657471,
      "learning_rate": 0.00011144444444444444,
      "loss": 1.3085,
      "step": 15150
    },
    {
      "epoch": 4.44454193171017,
      "grad_norm": 0.8538522720336914,
      "learning_rate": 0.00011115204678362574,
      "loss": 1.2209,
      "step": 15200
    },
    {
      "epoch": 4.459165021569057,
      "grad_norm": 1.1059821844100952,
      "learning_rate": 0.00011085964912280702,
      "loss": 1.3779,
      "step": 15250
    },
    {
      "epoch": 4.473788111427945,
      "grad_norm": 0.9469345211982727,
      "learning_rate": 0.00011056725146198832,
      "loss": 1.2775,
      "step": 15300
    },
    {
      "epoch": 4.488411201286832,
      "grad_norm": 0.9372389912605286,
      "learning_rate": 0.0001102748538011696,
      "loss": 1.3249,
      "step": 15350
    },
    {
      "epoch": 4.503034291145719,
      "grad_norm": 0.8753170967102051,
      "learning_rate": 0.00010998245614035088,
      "loss": 1.327,
      "step": 15400
    },
    {
      "epoch": 4.517657381004606,
      "grad_norm": 0.8182927370071411,
      "learning_rate": 0.00010969005847953217,
      "loss": 1.2606,
      "step": 15450
    },
    {
      "epoch": 4.532280470863493,
      "grad_norm": 1.174672245979309,
      "learning_rate": 0.00010939766081871345,
      "loss": 1.3141,
      "step": 15500
    },
    {
      "epoch": 4.546903560722381,
      "grad_norm": 0.8760483264923096,
      "learning_rate": 0.00010910526315789473,
      "loss": 1.2455,
      "step": 15550
    },
    {
      "epoch": 4.561526650581268,
      "grad_norm": 0.7917004227638245,
      "learning_rate": 0.00010881286549707603,
      "loss": 1.3619,
      "step": 15600
    },
    {
      "epoch": 4.5761497404401545,
      "grad_norm": 0.9179386496543884,
      "learning_rate": 0.00010852046783625731,
      "loss": 1.287,
      "step": 15650
    },
    {
      "epoch": 4.590772830299042,
      "grad_norm": 0.8143192529678345,
      "learning_rate": 0.00010822807017543862,
      "loss": 1.3408,
      "step": 15700
    },
    {
      "epoch": 4.605395920157929,
      "grad_norm": 0.9705249667167664,
      "learning_rate": 0.0001079356725146199,
      "loss": 1.295,
      "step": 15750
    },
    {
      "epoch": 4.620019010016817,
      "grad_norm": 0.9861381649971008,
      "learning_rate": 0.00010764327485380116,
      "loss": 1.2989,
      "step": 15800
    },
    {
      "epoch": 4.634642099875704,
      "grad_norm": 0.911867082118988,
      "learning_rate": 0.00010735087719298247,
      "loss": 1.3464,
      "step": 15850
    },
    {
      "epoch": 4.6492651897345905,
      "grad_norm": 1.0045251846313477,
      "learning_rate": 0.00010705847953216375,
      "loss": 1.3419,
      "step": 15900
    },
    {
      "epoch": 4.663888279593478,
      "grad_norm": 0.8566990494728088,
      "learning_rate": 0.00010676608187134503,
      "loss": 1.3059,
      "step": 15950
    },
    {
      "epoch": 4.678511369452365,
      "grad_norm": 0.7842316031455994,
      "learning_rate": 0.00010647368421052633,
      "loss": 1.3154,
      "step": 16000
    },
    {
      "epoch": 4.693134459311253,
      "grad_norm": 0.741337239742279,
      "learning_rate": 0.0001061812865497076,
      "loss": 1.2664,
      "step": 16050
    },
    {
      "epoch": 4.7077575491701396,
      "grad_norm": 1.0101885795593262,
      "learning_rate": 0.0001058888888888889,
      "loss": 1.2824,
      "step": 16100
    },
    {
      "epoch": 4.722380639029026,
      "grad_norm": 0.8165177702903748,
      "learning_rate": 0.00010559649122807018,
      "loss": 1.3279,
      "step": 16150
    },
    {
      "epoch": 4.737003728887914,
      "grad_norm": 1.1413443088531494,
      "learning_rate": 0.00010530409356725146,
      "loss": 1.3465,
      "step": 16200
    },
    {
      "epoch": 4.751626818746801,
      "grad_norm": 0.7626078128814697,
      "learning_rate": 0.00010501169590643275,
      "loss": 1.3653,
      "step": 16250
    },
    {
      "epoch": 4.766249908605689,
      "grad_norm": 1.1812937259674072,
      "learning_rate": 0.00010471929824561404,
      "loss": 1.3607,
      "step": 16300
    },
    {
      "epoch": 4.7808729984645755,
      "grad_norm": 0.865020215511322,
      "learning_rate": 0.00010442690058479532,
      "loss": 1.3149,
      "step": 16350
    },
    {
      "epoch": 4.795496088323462,
      "grad_norm": 1.267527461051941,
      "learning_rate": 0.00010413450292397662,
      "loss": 1.2475,
      "step": 16400
    },
    {
      "epoch": 4.81011917818235,
      "grad_norm": 0.9091158509254456,
      "learning_rate": 0.00010384210526315789,
      "loss": 1.2226,
      "step": 16450
    },
    {
      "epoch": 4.824742268041237,
      "grad_norm": 0.8514280319213867,
      "learning_rate": 0.0001035497076023392,
      "loss": 1.2684,
      "step": 16500
    },
    {
      "epoch": 4.839365357900125,
      "grad_norm": 1.0592621564865112,
      "learning_rate": 0.00010325730994152048,
      "loss": 1.3135,
      "step": 16550
    },
    {
      "epoch": 4.853988447759011,
      "grad_norm": 0.9694797992706299,
      "learning_rate": 0.00010296491228070176,
      "loss": 1.3254,
      "step": 16600
    },
    {
      "epoch": 4.868611537617898,
      "grad_norm": 0.9742067456245422,
      "learning_rate": 0.00010267251461988305,
      "loss": 1.3069,
      "step": 16650
    },
    {
      "epoch": 4.883234627476786,
      "grad_norm": 1.3129594326019287,
      "learning_rate": 0.00010238011695906433,
      "loss": 1.2844,
      "step": 16700
    },
    {
      "epoch": 4.897857717335673,
      "grad_norm": 0.9950486421585083,
      "learning_rate": 0.00010208771929824561,
      "loss": 1.2709,
      "step": 16750
    },
    {
      "epoch": 4.9124808071945605,
      "grad_norm": 0.6876882314682007,
      "learning_rate": 0.00010179532163742691,
      "loss": 1.3076,
      "step": 16800
    },
    {
      "epoch": 4.927103897053447,
      "grad_norm": 0.962415337562561,
      "learning_rate": 0.00010150292397660819,
      "loss": 1.2708,
      "step": 16850
    },
    {
      "epoch": 4.941726986912334,
      "grad_norm": 0.8351467251777649,
      "learning_rate": 0.00010121052631578948,
      "loss": 1.3464,
      "step": 16900
    },
    {
      "epoch": 4.956350076771222,
      "grad_norm": 0.9346233010292053,
      "learning_rate": 0.00010091812865497076,
      "loss": 1.2967,
      "step": 16950
    },
    {
      "epoch": 4.970973166630109,
      "grad_norm": 0.922947108745575,
      "learning_rate": 0.00010062573099415204,
      "loss": 1.2946,
      "step": 17000
    },
    {
      "epoch": 4.985596256488996,
      "grad_norm": 0.9670758843421936,
      "learning_rate": 0.00010033333333333335,
      "loss": 1.2734,
      "step": 17050
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.972567081451416,
      "learning_rate": 0.00010004093567251462,
      "loss": 1.3194,
      "step": 17100
    },
    {
      "epoch": 5.014623089858887,
      "grad_norm": 1.1338354349136353,
      "learning_rate": 9.974853801169591e-05,
      "loss": 1.2962,
      "step": 17150
    },
    {
      "epoch": 5.0292461797177745,
      "grad_norm": 1.2629363536834717,
      "learning_rate": 9.94561403508772e-05,
      "loss": 1.2364,
      "step": 17200
    },
    {
      "epoch": 5.043869269576661,
      "grad_norm": 1.0196832418441772,
      "learning_rate": 9.916374269005849e-05,
      "loss": 1.2325,
      "step": 17250
    },
    {
      "epoch": 5.058492359435549,
      "grad_norm": 1.0519435405731201,
      "learning_rate": 9.887134502923977e-05,
      "loss": 1.2827,
      "step": 17300
    },
    {
      "epoch": 5.073115449294436,
      "grad_norm": 1.1366885900497437,
      "learning_rate": 9.857894736842106e-05,
      "loss": 1.2864,
      "step": 17350
    },
    {
      "epoch": 5.087738539153323,
      "grad_norm": 0.961285412311554,
      "learning_rate": 9.828654970760235e-05,
      "loss": 1.2357,
      "step": 17400
    },
    {
      "epoch": 5.1023616290122105,
      "grad_norm": 0.5981482267379761,
      "learning_rate": 9.799415204678362e-05,
      "loss": 1.3123,
      "step": 17450
    },
    {
      "epoch": 5.116984718871097,
      "grad_norm": 1.0000554323196411,
      "learning_rate": 9.770175438596492e-05,
      "loss": 1.2503,
      "step": 17500
    },
    {
      "epoch": 5.131607808729985,
      "grad_norm": 0.8523291349411011,
      "learning_rate": 9.740935672514621e-05,
      "loss": 1.2691,
      "step": 17550
    },
    {
      "epoch": 5.146230898588872,
      "grad_norm": 0.6868172883987427,
      "learning_rate": 9.711695906432749e-05,
      "loss": 1.267,
      "step": 17600
    },
    {
      "epoch": 5.160853988447759,
      "grad_norm": 1.1729817390441895,
      "learning_rate": 9.682456140350877e-05,
      "loss": 1.3009,
      "step": 17650
    },
    {
      "epoch": 5.175477078306646,
      "grad_norm": 0.935011088848114,
      "learning_rate": 9.653216374269006e-05,
      "loss": 1.2926,
      "step": 17700
    },
    {
      "epoch": 5.190100168165533,
      "grad_norm": 1.0320625305175781,
      "learning_rate": 9.623976608187136e-05,
      "loss": 1.2729,
      "step": 17750
    },
    {
      "epoch": 5.204723258024421,
      "grad_norm": 1.0697362422943115,
      "learning_rate": 9.594736842105264e-05,
      "loss": 1.2951,
      "step": 17800
    },
    {
      "epoch": 5.219346347883308,
      "grad_norm": 1.1695048809051514,
      "learning_rate": 9.565497076023392e-05,
      "loss": 1.2722,
      "step": 17850
    },
    {
      "epoch": 5.233969437742195,
      "grad_norm": 1.2484861612319946,
      "learning_rate": 9.536257309941521e-05,
      "loss": 1.3386,
      "step": 17900
    },
    {
      "epoch": 5.248592527601082,
      "grad_norm": 0.9291399717330933,
      "learning_rate": 9.50701754385965e-05,
      "loss": 1.3163,
      "step": 17950
    },
    {
      "epoch": 5.263215617459969,
      "grad_norm": 1.3067777156829834,
      "learning_rate": 9.477777777777779e-05,
      "loss": 1.3256,
      "step": 18000
    },
    {
      "epoch": 5.277838707318857,
      "grad_norm": 0.9883979558944702,
      "learning_rate": 9.448538011695907e-05,
      "loss": 1.2998,
      "step": 18050
    },
    {
      "epoch": 5.292461797177744,
      "grad_norm": 0.9972461462020874,
      "learning_rate": 9.419298245614035e-05,
      "loss": 1.2479,
      "step": 18100
    },
    {
      "epoch": 5.3070848870366305,
      "grad_norm": 1.7350249290466309,
      "learning_rate": 9.390058479532164e-05,
      "loss": 1.2824,
      "step": 18150
    },
    {
      "epoch": 5.321707976895518,
      "grad_norm": 0.8066009283065796,
      "learning_rate": 9.360818713450294e-05,
      "loss": 1.2617,
      "step": 18200
    },
    {
      "epoch": 5.336331066754405,
      "grad_norm": 0.8733106851577759,
      "learning_rate": 9.331578947368422e-05,
      "loss": 1.2477,
      "step": 18250
    },
    {
      "epoch": 5.350954156613293,
      "grad_norm": 1.1703588962554932,
      "learning_rate": 9.30233918128655e-05,
      "loss": 1.3274,
      "step": 18300
    },
    {
      "epoch": 5.36557724647218,
      "grad_norm": 0.9179836511611938,
      "learning_rate": 9.273099415204679e-05,
      "loss": 1.2898,
      "step": 18350
    },
    {
      "epoch": 5.380200336331066,
      "grad_norm": 1.1092512607574463,
      "learning_rate": 9.243859649122809e-05,
      "loss": 1.3175,
      "step": 18400
    },
    {
      "epoch": 5.394823426189954,
      "grad_norm": 1.2620121240615845,
      "learning_rate": 9.214619883040935e-05,
      "loss": 1.2846,
      "step": 18450
    },
    {
      "epoch": 5.409446516048841,
      "grad_norm": 1.2325602769851685,
      "learning_rate": 9.185380116959065e-05,
      "loss": 1.26,
      "step": 18500
    },
    {
      "epoch": 5.424069605907729,
      "grad_norm": 1.0272858142852783,
      "learning_rate": 9.156725146198832e-05,
      "loss": 1.2409,
      "step": 18550
    },
    {
      "epoch": 5.4386926957666155,
      "grad_norm": 1.2042036056518555,
      "learning_rate": 9.12748538011696e-05,
      "loss": 1.3026,
      "step": 18600
    },
    {
      "epoch": 5.453315785625502,
      "grad_norm": 0.9609649181365967,
      "learning_rate": 9.098245614035088e-05,
      "loss": 1.251,
      "step": 18650
    },
    {
      "epoch": 5.46793887548439,
      "grad_norm": 0.863191545009613,
      "learning_rate": 9.069005847953217e-05,
      "loss": 1.2827,
      "step": 18700
    },
    {
      "epoch": 5.482561965343277,
      "grad_norm": 1.1652936935424805,
      "learning_rate": 9.039766081871347e-05,
      "loss": 1.322,
      "step": 18750
    },
    {
      "epoch": 5.497185055202165,
      "grad_norm": 0.9215826988220215,
      "learning_rate": 9.010526315789473e-05,
      "loss": 1.307,
      "step": 18800
    },
    {
      "epoch": 5.511808145061051,
      "grad_norm": 0.769544780254364,
      "learning_rate": 8.981286549707603e-05,
      "loss": 1.2717,
      "step": 18850
    },
    {
      "epoch": 5.526431234919938,
      "grad_norm": 0.9601797461509705,
      "learning_rate": 8.952046783625732e-05,
      "loss": 1.3552,
      "step": 18900
    },
    {
      "epoch": 5.541054324778826,
      "grad_norm": 1.125779151916504,
      "learning_rate": 8.92280701754386e-05,
      "loss": 1.3022,
      "step": 18950
    },
    {
      "epoch": 5.555677414637713,
      "grad_norm": 1.129500150680542,
      "learning_rate": 8.893567251461988e-05,
      "loss": 1.2558,
      "step": 19000
    },
    {
      "epoch": 5.5703005044966005,
      "grad_norm": 1.0752673149108887,
      "learning_rate": 8.864327485380118e-05,
      "loss": 1.2758,
      "step": 19050
    },
    {
      "epoch": 5.584923594355487,
      "grad_norm": 0.7709877490997314,
      "learning_rate": 8.835087719298246e-05,
      "loss": 1.3352,
      "step": 19100
    },
    {
      "epoch": 5.599546684214374,
      "grad_norm": 1.1928220987319946,
      "learning_rate": 8.805847953216375e-05,
      "loss": 1.3066,
      "step": 19150
    },
    {
      "epoch": 5.614169774073262,
      "grad_norm": 1.0924400091171265,
      "learning_rate": 8.776608187134503e-05,
      "loss": 1.2914,
      "step": 19200
    },
    {
      "epoch": 5.628792863932149,
      "grad_norm": 0.8618689775466919,
      "learning_rate": 8.747368421052632e-05,
      "loss": 1.2735,
      "step": 19250
    },
    {
      "epoch": 5.643415953791036,
      "grad_norm": 0.8869754076004028,
      "learning_rate": 8.71812865497076e-05,
      "loss": 1.3082,
      "step": 19300
    },
    {
      "epoch": 5.658039043649923,
      "grad_norm": 1.4617786407470703,
      "learning_rate": 8.68888888888889e-05,
      "loss": 1.2323,
      "step": 19350
    },
    {
      "epoch": 5.67266213350881,
      "grad_norm": 1.0052640438079834,
      "learning_rate": 8.659649122807018e-05,
      "loss": 1.3046,
      "step": 19400
    },
    {
      "epoch": 5.687285223367698,
      "grad_norm": 1.0225164890289307,
      "learning_rate": 8.630409356725146e-05,
      "loss": 1.3684,
      "step": 19450
    },
    {
      "epoch": 5.701908313226585,
      "grad_norm": 1.079568862915039,
      "learning_rate": 8.601169590643275e-05,
      "loss": 1.3425,
      "step": 19500
    },
    {
      "epoch": 5.716531403085472,
      "grad_norm": 0.7566020488739014,
      "learning_rate": 8.571929824561405e-05,
      "loss": 1.3084,
      "step": 19550
    },
    {
      "epoch": 5.731154492944359,
      "grad_norm": 0.802224338054657,
      "learning_rate": 8.542690058479533e-05,
      "loss": 1.2338,
      "step": 19600
    },
    {
      "epoch": 5.745777582803246,
      "grad_norm": 0.9764975309371948,
      "learning_rate": 8.513450292397661e-05,
      "loss": 1.2992,
      "step": 19650
    },
    {
      "epoch": 5.760400672662134,
      "grad_norm": 0.7221105694770813,
      "learning_rate": 8.484795321637428e-05,
      "loss": 1.2821,
      "step": 19700
    },
    {
      "epoch": 5.775023762521021,
      "grad_norm": 0.8709716796875,
      "learning_rate": 8.455555555555556e-05,
      "loss": 1.2763,
      "step": 19750
    },
    {
      "epoch": 5.789646852379908,
      "grad_norm": 0.8697527647018433,
      "learning_rate": 8.426315789473684e-05,
      "loss": 1.2436,
      "step": 19800
    },
    {
      "epoch": 5.804269942238795,
      "grad_norm": 0.8975472450256348,
      "learning_rate": 8.397076023391813e-05,
      "loss": 1.2895,
      "step": 19850
    },
    {
      "epoch": 5.818893032097682,
      "grad_norm": 1.0334478616714478,
      "learning_rate": 8.367836257309943e-05,
      "loss": 1.3105,
      "step": 19900
    },
    {
      "epoch": 5.83351612195657,
      "grad_norm": 1.1875832080841064,
      "learning_rate": 8.338596491228071e-05,
      "loss": 1.2087,
      "step": 19950
    },
    {
      "epoch": 5.8481392118154565,
      "grad_norm": 0.9904586672782898,
      "learning_rate": 8.309356725146199e-05,
      "loss": 1.3149,
      "step": 20000
    },
    {
      "epoch": 5.862762301674344,
      "grad_norm": 0.7263789772987366,
      "learning_rate": 8.280116959064328e-05,
      "loss": 1.2981,
      "step": 20050
    },
    {
      "epoch": 5.877385391533231,
      "grad_norm": 0.9120916128158569,
      "learning_rate": 8.250877192982456e-05,
      "loss": 1.2343,
      "step": 20100
    },
    {
      "epoch": 5.892008481392118,
      "grad_norm": 0.9014617204666138,
      "learning_rate": 8.221637426900584e-05,
      "loss": 1.3001,
      "step": 20150
    },
    {
      "epoch": 5.906631571251006,
      "grad_norm": 1.384522795677185,
      "learning_rate": 8.192397660818714e-05,
      "loss": 1.3281,
      "step": 20200
    },
    {
      "epoch": 5.921254661109892,
      "grad_norm": 1.1891690492630005,
      "learning_rate": 8.163157894736843e-05,
      "loss": 1.2921,
      "step": 20250
    },
    {
      "epoch": 5.93587775096878,
      "grad_norm": 1.1444990634918213,
      "learning_rate": 8.133918128654971e-05,
      "loss": 1.2972,
      "step": 20300
    },
    {
      "epoch": 5.950500840827667,
      "grad_norm": 0.7462369203567505,
      "learning_rate": 8.104678362573099e-05,
      "loss": 1.3181,
      "step": 20350
    },
    {
      "epoch": 5.965123930686554,
      "grad_norm": 1.1595487594604492,
      "learning_rate": 8.075438596491229e-05,
      "loss": 1.2698,
      "step": 20400
    },
    {
      "epoch": 5.9797470205454415,
      "grad_norm": 0.9436349272727966,
      "learning_rate": 8.046198830409357e-05,
      "loss": 1.2959,
      "step": 20450
    },
    {
      "epoch": 5.994370110404328,
      "grad_norm": 1.0019505023956299,
      "learning_rate": 8.016959064327486e-05,
      "loss": 1.3131,
      "step": 20500
    },
    {
      "epoch": 6.008773853915332,
      "grad_norm": 0.7590315937995911,
      "learning_rate": 7.987719298245614e-05,
      "loss": 1.2632,
      "step": 20550
    },
    {
      "epoch": 6.02339694377422,
      "grad_norm": 1.1455519199371338,
      "learning_rate": 7.958479532163743e-05,
      "loss": 1.3414,
      "step": 20600
    },
    {
      "epoch": 6.0380200336331065,
      "grad_norm": 0.7694852352142334,
      "learning_rate": 7.929239766081872e-05,
      "loss": 1.3556,
      "step": 20650
    },
    {
      "epoch": 6.052643123491994,
      "grad_norm": 1.1094738245010376,
      "learning_rate": 7.900000000000001e-05,
      "loss": 1.2475,
      "step": 20700
    },
    {
      "epoch": 6.067266213350881,
      "grad_norm": 0.8128076195716858,
      "learning_rate": 7.870760233918129e-05,
      "loss": 1.2882,
      "step": 20750
    },
    {
      "epoch": 6.081889303209768,
      "grad_norm": 0.7095981240272522,
      "learning_rate": 7.841520467836257e-05,
      "loss": 1.2038,
      "step": 20800
    },
    {
      "epoch": 6.0965123930686556,
      "grad_norm": 0.704900860786438,
      "learning_rate": 7.812280701754386e-05,
      "loss": 1.219,
      "step": 20850
    },
    {
      "epoch": 6.111135482927542,
      "grad_norm": 1.4137099981307983,
      "learning_rate": 7.783040935672516e-05,
      "loss": 1.2917,
      "step": 20900
    },
    {
      "epoch": 6.12575857278643,
      "grad_norm": 0.9490087628364563,
      "learning_rate": 7.753801169590643e-05,
      "loss": 1.2628,
      "step": 20950
    },
    {
      "epoch": 6.140381662645317,
      "grad_norm": 0.7084046602249146,
      "learning_rate": 7.724561403508772e-05,
      "loss": 1.2629,
      "step": 21000
    },
    {
      "epoch": 6.155004752504204,
      "grad_norm": 0.8842262029647827,
      "learning_rate": 7.695321637426901e-05,
      "loss": 1.225,
      "step": 21050
    },
    {
      "epoch": 6.1696278423630915,
      "grad_norm": 1.1673024892807007,
      "learning_rate": 7.66608187134503e-05,
      "loss": 1.3062,
      "step": 21100
    },
    {
      "epoch": 6.184250932221978,
      "grad_norm": 1.1846448183059692,
      "learning_rate": 7.636842105263157e-05,
      "loss": 1.3223,
      "step": 21150
    },
    {
      "epoch": 6.198874022080866,
      "grad_norm": 0.8045490980148315,
      "learning_rate": 7.607602339181287e-05,
      "loss": 1.2657,
      "step": 21200
    },
    {
      "epoch": 6.213497111939753,
      "grad_norm": 1.4077523946762085,
      "learning_rate": 7.578362573099416e-05,
      "loss": 1.2448,
      "step": 21250
    },
    {
      "epoch": 6.22812020179864,
      "grad_norm": 1.0770611763000488,
      "learning_rate": 7.549122807017544e-05,
      "loss": 1.218,
      "step": 21300
    },
    {
      "epoch": 6.242743291657527,
      "grad_norm": 0.8021659851074219,
      "learning_rate": 7.519883040935672e-05,
      "loss": 1.2952,
      "step": 21350
    },
    {
      "epoch": 6.257366381516414,
      "grad_norm": 1.1478080749511719,
      "learning_rate": 7.490643274853802e-05,
      "loss": 1.2473,
      "step": 21400
    },
    {
      "epoch": 6.271989471375302,
      "grad_norm": 1.097494125366211,
      "learning_rate": 7.46140350877193e-05,
      "loss": 1.238,
      "step": 21450
    },
    {
      "epoch": 6.286612561234189,
      "grad_norm": 1.2004473209381104,
      "learning_rate": 7.432163742690059e-05,
      "loss": 1.2374,
      "step": 21500
    },
    {
      "epoch": 6.301235651093076,
      "grad_norm": 1.065762996673584,
      "learning_rate": 7.402923976608187e-05,
      "loss": 1.2429,
      "step": 21550
    },
    {
      "epoch": 6.315858740951963,
      "grad_norm": 1.1599855422973633,
      "learning_rate": 7.373684210526317e-05,
      "loss": 1.2806,
      "step": 21600
    },
    {
      "epoch": 6.33048183081085,
      "grad_norm": 1.0537400245666504,
      "learning_rate": 7.344444444444445e-05,
      "loss": 1.2453,
      "step": 21650
    },
    {
      "epoch": 6.345104920669738,
      "grad_norm": 1.2915152311325073,
      "learning_rate": 7.315204678362574e-05,
      "loss": 1.2869,
      "step": 21700
    },
    {
      "epoch": 6.359728010528625,
      "grad_norm": 1.2831944227218628,
      "learning_rate": 7.285964912280702e-05,
      "loss": 1.3111,
      "step": 21750
    },
    {
      "epoch": 6.3743511003875115,
      "grad_norm": 1.3025213479995728,
      "learning_rate": 7.25672514619883e-05,
      "loss": 1.2152,
      "step": 21800
    },
    {
      "epoch": 6.388974190246399,
      "grad_norm": 1.229970097541809,
      "learning_rate": 7.22748538011696e-05,
      "loss": 1.2433,
      "step": 21850
    },
    {
      "epoch": 6.403597280105286,
      "grad_norm": 1.1933419704437256,
      "learning_rate": 7.198245614035089e-05,
      "loss": 1.3171,
      "step": 21900
    },
    {
      "epoch": 6.418220369964174,
      "grad_norm": 0.9752159714698792,
      "learning_rate": 7.169005847953216e-05,
      "loss": 1.2708,
      "step": 21950
    },
    {
      "epoch": 6.432843459823061,
      "grad_norm": 1.3210809230804443,
      "learning_rate": 7.140350877192983e-05,
      "loss": 1.2019,
      "step": 22000
    },
    {
      "epoch": 6.447466549681947,
      "grad_norm": 1.3075605630874634,
      "learning_rate": 7.111111111111112e-05,
      "loss": 1.3186,
      "step": 22050
    },
    {
      "epoch": 6.462089639540835,
      "grad_norm": 0.9989105463027954,
      "learning_rate": 7.08187134502924e-05,
      "loss": 1.2932,
      "step": 22100
    },
    {
      "epoch": 6.476712729399722,
      "grad_norm": 0.8548855185508728,
      "learning_rate": 7.052631578947368e-05,
      "loss": 1.2997,
      "step": 22150
    },
    {
      "epoch": 6.49133581925861,
      "grad_norm": 0.7118768692016602,
      "learning_rate": 7.023391812865497e-05,
      "loss": 1.2324,
      "step": 22200
    },
    {
      "epoch": 6.5059589091174965,
      "grad_norm": 0.9189091920852661,
      "learning_rate": 6.994152046783627e-05,
      "loss": 1.3,
      "step": 22250
    },
    {
      "epoch": 6.520581998976383,
      "grad_norm": 1.2628867626190186,
      "learning_rate": 6.964912280701754e-05,
      "loss": 1.3016,
      "step": 22300
    },
    {
      "epoch": 6.535205088835271,
      "grad_norm": 1.0771510601043701,
      "learning_rate": 6.935672514619883e-05,
      "loss": 1.2512,
      "step": 22350
    },
    {
      "epoch": 6.549828178694158,
      "grad_norm": 1.1676568984985352,
      "learning_rate": 6.906432748538012e-05,
      "loss": 1.1953,
      "step": 22400
    },
    {
      "epoch": 6.564451268553046,
      "grad_norm": 1.0584052801132202,
      "learning_rate": 6.87719298245614e-05,
      "loss": 1.2354,
      "step": 22450
    },
    {
      "epoch": 6.579074358411932,
      "grad_norm": 1.2493245601654053,
      "learning_rate": 6.847953216374268e-05,
      "loss": 1.3196,
      "step": 22500
    },
    {
      "epoch": 6.593697448270819,
      "grad_norm": 1.2406820058822632,
      "learning_rate": 6.818713450292398e-05,
      "loss": 1.2908,
      "step": 22550
    },
    {
      "epoch": 6.608320538129707,
      "grad_norm": 1.1249563694000244,
      "learning_rate": 6.789473684210527e-05,
      "loss": 1.2899,
      "step": 22600
    },
    {
      "epoch": 6.622943627988594,
      "grad_norm": 1.0605392456054688,
      "learning_rate": 6.760233918128655e-05,
      "loss": 1.2996,
      "step": 22650
    },
    {
      "epoch": 6.6375667178474815,
      "grad_norm": 1.303161382675171,
      "learning_rate": 6.730994152046783e-05,
      "loss": 1.3123,
      "step": 22700
    },
    {
      "epoch": 6.652189807706368,
      "grad_norm": 1.038986325263977,
      "learning_rate": 6.701754385964913e-05,
      "loss": 1.2737,
      "step": 22750
    },
    {
      "epoch": 6.666812897565255,
      "grad_norm": 1.1040548086166382,
      "learning_rate": 6.672514619883041e-05,
      "loss": 1.1657,
      "step": 22800
    },
    {
      "epoch": 6.681435987424143,
      "grad_norm": 1.086952805519104,
      "learning_rate": 6.64327485380117e-05,
      "loss": 1.2465,
      "step": 22850
    },
    {
      "epoch": 6.69605907728303,
      "grad_norm": 1.0924690961837769,
      "learning_rate": 6.614035087719298e-05,
      "loss": 1.3296,
      "step": 22900
    },
    {
      "epoch": 6.7106821671419175,
      "grad_norm": 1.2734073400497437,
      "learning_rate": 6.584795321637426e-05,
      "loss": 1.3119,
      "step": 22950
    },
    {
      "epoch": 6.725305257000804,
      "grad_norm": 0.9579964280128479,
      "learning_rate": 6.555555555555556e-05,
      "loss": 1.2172,
      "step": 23000
    },
    {
      "epoch": 6.739928346859691,
      "grad_norm": 0.8606013655662537,
      "learning_rate": 6.526315789473685e-05,
      "loss": 1.2635,
      "step": 23050
    },
    {
      "epoch": 6.754551436718579,
      "grad_norm": 0.8781128525733948,
      "learning_rate": 6.497076023391813e-05,
      "loss": 1.3265,
      "step": 23100
    },
    {
      "epoch": 6.769174526577466,
      "grad_norm": 1.4043078422546387,
      "learning_rate": 6.467836257309941e-05,
      "loss": 1.2518,
      "step": 23150
    },
    {
      "epoch": 6.783797616436353,
      "grad_norm": 2.0289950370788574,
      "learning_rate": 6.43859649122807e-05,
      "loss": 1.2545,
      "step": 23200
    },
    {
      "epoch": 6.79842070629524,
      "grad_norm": 1.2324882745742798,
      "learning_rate": 6.4093567251462e-05,
      "loss": 1.2609,
      "step": 23250
    },
    {
      "epoch": 6.813043796154127,
      "grad_norm": 1.0602596998214722,
      "learning_rate": 6.380116959064327e-05,
      "loss": 1.2284,
      "step": 23300
    },
    {
      "epoch": 6.827666886013015,
      "grad_norm": 1.151533842086792,
      "learning_rate": 6.350877192982456e-05,
      "loss": 1.2808,
      "step": 23350
    },
    {
      "epoch": 6.842289975871902,
      "grad_norm": 0.8725298643112183,
      "learning_rate": 6.321637426900586e-05,
      "loss": 1.2808,
      "step": 23400
    },
    {
      "epoch": 6.856913065730789,
      "grad_norm": 1.1586593389511108,
      "learning_rate": 6.292397660818714e-05,
      "loss": 1.2348,
      "step": 23450
    },
    {
      "epoch": 6.871536155589676,
      "grad_norm": 0.9869416952133179,
      "learning_rate": 6.263157894736842e-05,
      "loss": 1.2893,
      "step": 23500
    },
    {
      "epoch": 6.886159245448563,
      "grad_norm": 1.2546987533569336,
      "learning_rate": 6.233918128654971e-05,
      "loss": 1.2611,
      "step": 23550
    },
    {
      "epoch": 6.900782335307451,
      "grad_norm": 0.8932939171791077,
      "learning_rate": 6.2046783625731e-05,
      "loss": 1.267,
      "step": 23600
    },
    {
      "epoch": 6.9154054251663375,
      "grad_norm": 0.9215221405029297,
      "learning_rate": 6.175438596491228e-05,
      "loss": 1.2804,
      "step": 23650
    },
    {
      "epoch": 6.930028515025225,
      "grad_norm": 1.0972871780395508,
      "learning_rate": 6.146198830409357e-05,
      "loss": 1.3763,
      "step": 23700
    },
    {
      "epoch": 6.944651604884112,
      "grad_norm": 1.0201469659805298,
      "learning_rate": 6.116959064327486e-05,
      "loss": 1.3142,
      "step": 23750
    },
    {
      "epoch": 6.959274694742999,
      "grad_norm": 1.4535236358642578,
      "learning_rate": 6.0877192982456147e-05,
      "loss": 1.323,
      "step": 23800
    },
    {
      "epoch": 6.973897784601887,
      "grad_norm": 1.0280444622039795,
      "learning_rate": 6.0584795321637434e-05,
      "loss": 1.2344,
      "step": 23850
    },
    {
      "epoch": 6.988520874460773,
      "grad_norm": 1.1224238872528076,
      "learning_rate": 6.0292397660818714e-05,
      "loss": 1.2895,
      "step": 23900
    },
    {
      "epoch": 7.002924617971777,
      "grad_norm": 1.05367112159729,
      "learning_rate": 6e-05,
      "loss": 1.287,
      "step": 23950
    },
    {
      "epoch": 7.017547707830665,
      "grad_norm": 0.7502979040145874,
      "learning_rate": 5.970760233918129e-05,
      "loss": 1.2915,
      "step": 24000
    },
    {
      "epoch": 7.0321707976895516,
      "grad_norm": 1.190462350845337,
      "learning_rate": 5.9415204678362576e-05,
      "loss": 1.2998,
      "step": 24050
    },
    {
      "epoch": 7.046793887548439,
      "grad_norm": 0.9946936368942261,
      "learning_rate": 5.9122807017543856e-05,
      "loss": 1.2437,
      "step": 24100
    },
    {
      "epoch": 7.061416977407326,
      "grad_norm": 1.0456087589263916,
      "learning_rate": 5.8830409356725144e-05,
      "loss": 1.2968,
      "step": 24150
    },
    {
      "epoch": 7.076040067266213,
      "grad_norm": 1.327192783355713,
      "learning_rate": 5.853801169590644e-05,
      "loss": 1.2595,
      "step": 24200
    },
    {
      "epoch": 7.090663157125101,
      "grad_norm": 1.0941786766052246,
      "learning_rate": 5.8245614035087725e-05,
      "loss": 1.2955,
      "step": 24250
    },
    {
      "epoch": 7.1052862469839875,
      "grad_norm": 1.026647925376892,
      "learning_rate": 5.7953216374269005e-05,
      "loss": 1.2707,
      "step": 24300
    },
    {
      "epoch": 7.119909336842875,
      "grad_norm": 1.3260093927383423,
      "learning_rate": 5.766081871345029e-05,
      "loss": 1.2379,
      "step": 24350
    },
    {
      "epoch": 7.134532426701762,
      "grad_norm": 1.17733633518219,
      "learning_rate": 5.736842105263158e-05,
      "loss": 1.2383,
      "step": 24400
    },
    {
      "epoch": 7.149155516560649,
      "grad_norm": 1.3268307447433472,
      "learning_rate": 5.7076023391812874e-05,
      "loss": 1.2458,
      "step": 24450
    },
    {
      "epoch": 7.163778606419537,
      "grad_norm": 1.7276519536972046,
      "learning_rate": 5.678362573099415e-05,
      "loss": 1.2218,
      "step": 24500
    },
    {
      "epoch": 7.178401696278423,
      "grad_norm": 1.1267768144607544,
      "learning_rate": 5.649122807017544e-05,
      "loss": 1.281,
      "step": 24550
    },
    {
      "epoch": 7.193024786137311,
      "grad_norm": 0.950983464717865,
      "learning_rate": 5.619883040935673e-05,
      "loss": 1.2899,
      "step": 24600
    },
    {
      "epoch": 7.207647875996198,
      "grad_norm": 1.4746785163879395,
      "learning_rate": 5.5906432748538016e-05,
      "loss": 1.2158,
      "step": 24650
    },
    {
      "epoch": 7.222270965855085,
      "grad_norm": 0.9300893545150757,
      "learning_rate": 5.56140350877193e-05,
      "loss": 1.3089,
      "step": 24700
    },
    {
      "epoch": 7.2368940557139725,
      "grad_norm": 1.0625730752944946,
      "learning_rate": 5.5321637426900584e-05,
      "loss": 1.2679,
      "step": 24750
    },
    {
      "epoch": 7.251517145572859,
      "grad_norm": 1.3324203491210938,
      "learning_rate": 5.502923976608188e-05,
      "loss": 1.2215,
      "step": 24800
    },
    {
      "epoch": 7.266140235431747,
      "grad_norm": 1.289404273033142,
      "learning_rate": 5.4736842105263165e-05,
      "loss": 1.2998,
      "step": 24850
    },
    {
      "epoch": 7.280763325290634,
      "grad_norm": 0.8145565390586853,
      "learning_rate": 5.4444444444444446e-05,
      "loss": 1.2123,
      "step": 24900
    },
    {
      "epoch": 7.295386415149521,
      "grad_norm": 1.5702751874923706,
      "learning_rate": 5.415204678362573e-05,
      "loss": 1.235,
      "step": 24950
    },
    {
      "epoch": 7.310009505008408,
      "grad_norm": 1.2003755569458008,
      "learning_rate": 5.385964912280702e-05,
      "loss": 1.2262,
      "step": 25000
    },
    {
      "epoch": 7.324632594867295,
      "grad_norm": 1.226656436920166,
      "learning_rate": 5.356725146198831e-05,
      "loss": 1.2934,
      "step": 25050
    },
    {
      "epoch": 7.339255684726183,
      "grad_norm": 0.938216507434845,
      "learning_rate": 5.327485380116959e-05,
      "loss": 1.2643,
      "step": 25100
    },
    {
      "epoch": 7.35387877458507,
      "grad_norm": 1.094576120376587,
      "learning_rate": 5.2982456140350875e-05,
      "loss": 1.2427,
      "step": 25150
    },
    {
      "epoch": 7.368501864443957,
      "grad_norm": 1.2696880102157593,
      "learning_rate": 5.269005847953217e-05,
      "loss": 1.1825,
      "step": 25200
    },
    {
      "epoch": 7.383124954302844,
      "grad_norm": 1.0810284614562988,
      "learning_rate": 5.2397660818713456e-05,
      "loss": 1.2299,
      "step": 25250
    },
    {
      "epoch": 7.397748044161731,
      "grad_norm": 1.0395604372024536,
      "learning_rate": 5.210526315789474e-05,
      "loss": 1.2206,
      "step": 25300
    },
    {
      "epoch": 7.412371134020619,
      "grad_norm": 0.9801236987113953,
      "learning_rate": 5.1812865497076024e-05,
      "loss": 1.2727,
      "step": 25350
    },
    {
      "epoch": 7.426994223879506,
      "grad_norm": 0.8726176023483276,
      "learning_rate": 5.152046783625731e-05,
      "loss": 1.2208,
      "step": 25400
    },
    {
      "epoch": 7.4416173137383925,
      "grad_norm": 1.2503831386566162,
      "learning_rate": 5.1228070175438605e-05,
      "loss": 1.2895,
      "step": 25450
    },
    {
      "epoch": 7.45624040359728,
      "grad_norm": 0.9764963388442993,
      "learning_rate": 5.093567251461988e-05,
      "loss": 1.2095,
      "step": 25500
    },
    {
      "epoch": 7.470863493456167,
      "grad_norm": 1.588969111442566,
      "learning_rate": 5.064327485380117e-05,
      "loss": 1.2311,
      "step": 25550
    },
    {
      "epoch": 7.485486583315055,
      "grad_norm": 1.0831470489501953,
      "learning_rate": 5.035087719298246e-05,
      "loss": 1.2713,
      "step": 25600
    },
    {
      "epoch": 7.500109673173942,
      "grad_norm": 0.9952852129936218,
      "learning_rate": 5.005847953216375e-05,
      "loss": 1.252,
      "step": 25650
    },
    {
      "epoch": 7.5147327630328284,
      "grad_norm": 0.8670432567596436,
      "learning_rate": 4.9766081871345035e-05,
      "loss": 1.3133,
      "step": 25700
    },
    {
      "epoch": 7.529355852891716,
      "grad_norm": 1.1226024627685547,
      "learning_rate": 4.9473684210526315e-05,
      "loss": 1.2289,
      "step": 25750
    },
    {
      "epoch": 7.543978942750603,
      "grad_norm": 1.3312770128250122,
      "learning_rate": 4.91812865497076e-05,
      "loss": 1.2599,
      "step": 25800
    },
    {
      "epoch": 7.558602032609491,
      "grad_norm": 1.0518203973770142,
      "learning_rate": 4.888888888888889e-05,
      "loss": 1.2423,
      "step": 25850
    },
    {
      "epoch": 7.5732251224683775,
      "grad_norm": 1.07833993434906,
      "learning_rate": 4.859649122807018e-05,
      "loss": 1.2518,
      "step": 25900
    },
    {
      "epoch": 7.587848212327264,
      "grad_norm": 1.0813966989517212,
      "learning_rate": 4.8304093567251464e-05,
      "loss": 1.2128,
      "step": 25950
    },
    {
      "epoch": 7.602471302186152,
      "grad_norm": 0.9317477345466614,
      "learning_rate": 4.801169590643275e-05,
      "loss": 1.3222,
      "step": 26000
    },
    {
      "epoch": 7.617094392045039,
      "grad_norm": 1.7545801401138306,
      "learning_rate": 4.7725146198830414e-05,
      "loss": 1.2489,
      "step": 26050
    },
    {
      "epoch": 7.631717481903927,
      "grad_norm": 1.0151176452636719,
      "learning_rate": 4.74327485380117e-05,
      "loss": 1.2646,
      "step": 26100
    },
    {
      "epoch": 7.6463405717628135,
      "grad_norm": 1.0779508352279663,
      "learning_rate": 4.714035087719298e-05,
      "loss": 1.2943,
      "step": 26150
    },
    {
      "epoch": 7.6609636616217,
      "grad_norm": 0.8558621406555176,
      "learning_rate": 4.684795321637427e-05,
      "loss": 1.2752,
      "step": 26200
    },
    {
      "epoch": 7.675586751480588,
      "grad_norm": 0.8053162097930908,
      "learning_rate": 4.6555555555555556e-05,
      "loss": 1.2726,
      "step": 26250
    },
    {
      "epoch": 7.690209841339475,
      "grad_norm": 1.1096240282058716,
      "learning_rate": 4.6263157894736844e-05,
      "loss": 1.2463,
      "step": 26300
    },
    {
      "epoch": 7.7048329311983625,
      "grad_norm": 1.0372110605239868,
      "learning_rate": 4.597076023391813e-05,
      "loss": 1.291,
      "step": 26350
    },
    {
      "epoch": 7.719456021057249,
      "grad_norm": 0.8420963883399963,
      "learning_rate": 4.567836257309942e-05,
      "loss": 1.2465,
      "step": 26400
    },
    {
      "epoch": 7.734079110916136,
      "grad_norm": 1.1782866716384888,
      "learning_rate": 4.5385964912280705e-05,
      "loss": 1.2233,
      "step": 26450
    },
    {
      "epoch": 7.748702200775024,
      "grad_norm": 0.9207600951194763,
      "learning_rate": 4.5093567251461986e-05,
      "loss": 1.2149,
      "step": 26500
    },
    {
      "epoch": 7.763325290633911,
      "grad_norm": 1.2774852514266968,
      "learning_rate": 4.480116959064328e-05,
      "loss": 1.28,
      "step": 26550
    },
    {
      "epoch": 7.7779483804927985,
      "grad_norm": 1.3018180131912231,
      "learning_rate": 4.450877192982456e-05,
      "loss": 1.2254,
      "step": 26600
    },
    {
      "epoch": 7.792571470351685,
      "grad_norm": 0.909749448299408,
      "learning_rate": 4.421637426900585e-05,
      "loss": 1.3046,
      "step": 26650
    },
    {
      "epoch": 7.807194560210572,
      "grad_norm": 1.2651749849319458,
      "learning_rate": 4.3923976608187135e-05,
      "loss": 1.3066,
      "step": 26700
    },
    {
      "epoch": 7.82181765006946,
      "grad_norm": 1.1470998525619507,
      "learning_rate": 4.363157894736842e-05,
      "loss": 1.3186,
      "step": 26750
    },
    {
      "epoch": 7.836440739928347,
      "grad_norm": 1.2871992588043213,
      "learning_rate": 4.333918128654971e-05,
      "loss": 1.2643,
      "step": 26800
    },
    {
      "epoch": 7.851063829787234,
      "grad_norm": 1.2132817506790161,
      "learning_rate": 4.3046783625730997e-05,
      "loss": 1.1754,
      "step": 26850
    },
    {
      "epoch": 7.865686919646121,
      "grad_norm": 0.7701047658920288,
      "learning_rate": 4.275438596491228e-05,
      "loss": 1.3266,
      "step": 26900
    },
    {
      "epoch": 7.880310009505008,
      "grad_norm": 0.9009009003639221,
      "learning_rate": 4.246198830409357e-05,
      "loss": 1.2728,
      "step": 26950
    },
    {
      "epoch": 7.894933099363896,
      "grad_norm": 1.24729323387146,
      "learning_rate": 4.216959064327485e-05,
      "loss": 1.2733,
      "step": 27000
    },
    {
      "epoch": 7.909556189222783,
      "grad_norm": 0.9504646062850952,
      "learning_rate": 4.1877192982456146e-05,
      "loss": 1.2821,
      "step": 27050
    },
    {
      "epoch": 7.92417927908167,
      "grad_norm": 0.9311625957489014,
      "learning_rate": 4.1584795321637426e-05,
      "loss": 1.3348,
      "step": 27100
    },
    {
      "epoch": 7.938802368940557,
      "grad_norm": 1.1803947687149048,
      "learning_rate": 4.129239766081871e-05,
      "loss": 1.2819,
      "step": 27150
    },
    {
      "epoch": 7.953425458799444,
      "grad_norm": 1.0060144662857056,
      "learning_rate": 4.1005847953216376e-05,
      "loss": 1.284,
      "step": 27200
    },
    {
      "epoch": 7.968048548658332,
      "grad_norm": 0.9860705137252808,
      "learning_rate": 4.071345029239766e-05,
      "loss": 1.272,
      "step": 27250
    },
    {
      "epoch": 7.9826716385172185,
      "grad_norm": 1.497389316558838,
      "learning_rate": 4.042105263157895e-05,
      "loss": 1.2852,
      "step": 27300
    },
    {
      "epoch": 7.997294728376106,
      "grad_norm": 1.1655329465866089,
      "learning_rate": 4.012865497076024e-05,
      "loss": 1.2248,
      "step": 27350
    },
    {
      "epoch": 8.01169847188711,
      "grad_norm": 0.9561967849731445,
      "learning_rate": 3.9836257309941525e-05,
      "loss": 1.1972,
      "step": 27400
    },
    {
      "epoch": 8.026321561745997,
      "grad_norm": 1.1744656562805176,
      "learning_rate": 3.954385964912281e-05,
      "loss": 1.2473,
      "step": 27450
    },
    {
      "epoch": 8.040944651604883,
      "grad_norm": 1.093540072441101,
      "learning_rate": 3.925146198830409e-05,
      "loss": 1.2479,
      "step": 27500
    },
    {
      "epoch": 8.055567741463772,
      "grad_norm": 1.128572940826416,
      "learning_rate": 3.8959064327485386e-05,
      "loss": 1.2423,
      "step": 27550
    },
    {
      "epoch": 8.070190831322659,
      "grad_norm": 1.278809905052185,
      "learning_rate": 3.866666666666667e-05,
      "loss": 1.1462,
      "step": 27600
    },
    {
      "epoch": 8.084813921181546,
      "grad_norm": 1.1495423316955566,
      "learning_rate": 3.8374269005847954e-05,
      "loss": 1.2288,
      "step": 27650
    },
    {
      "epoch": 8.099437011040433,
      "grad_norm": 1.0931041240692139,
      "learning_rate": 3.808187134502924e-05,
      "loss": 1.265,
      "step": 27700
    },
    {
      "epoch": 8.11406010089932,
      "grad_norm": 0.9247224926948547,
      "learning_rate": 3.778947368421053e-05,
      "loss": 1.2512,
      "step": 27750
    },
    {
      "epoch": 8.128683190758208,
      "grad_norm": 1.201791524887085,
      "learning_rate": 3.7497076023391816e-05,
      "loss": 1.2771,
      "step": 27800
    },
    {
      "epoch": 8.143306280617095,
      "grad_norm": 1.0790557861328125,
      "learning_rate": 3.72046783625731e-05,
      "loss": 1.2025,
      "step": 27850
    },
    {
      "epoch": 8.157929370475982,
      "grad_norm": 1.287946105003357,
      "learning_rate": 3.691228070175439e-05,
      "loss": 1.1939,
      "step": 27900
    },
    {
      "epoch": 8.172552460334868,
      "grad_norm": 1.0756404399871826,
      "learning_rate": 3.661988304093568e-05,
      "loss": 1.2267,
      "step": 27950
    },
    {
      "epoch": 8.187175550193755,
      "grad_norm": 1.0283869504928589,
      "learning_rate": 3.632748538011696e-05,
      "loss": 1.21,
      "step": 28000
    },
    {
      "epoch": 8.201798640052644,
      "grad_norm": 1.0219517946243286,
      "learning_rate": 3.603508771929825e-05,
      "loss": 1.2264,
      "step": 28050
    },
    {
      "epoch": 8.21642172991153,
      "grad_norm": 1.2008044719696045,
      "learning_rate": 3.574269005847953e-05,
      "loss": 1.2411,
      "step": 28100
    },
    {
      "epoch": 8.231044819770418,
      "grad_norm": 1.1669176816940308,
      "learning_rate": 3.545029239766082e-05,
      "loss": 1.2249,
      "step": 28150
    },
    {
      "epoch": 8.245667909629304,
      "grad_norm": 0.7720693945884705,
      "learning_rate": 3.515789473684211e-05,
      "loss": 1.212,
      "step": 28200
    },
    {
      "epoch": 8.260290999488191,
      "grad_norm": 1.2302982807159424,
      "learning_rate": 3.4865497076023394e-05,
      "loss": 1.2913,
      "step": 28250
    },
    {
      "epoch": 8.27491408934708,
      "grad_norm": 1.3307987451553345,
      "learning_rate": 3.457309941520468e-05,
      "loss": 1.2981,
      "step": 28300
    },
    {
      "epoch": 8.289537179205967,
      "grad_norm": 1.4619743824005127,
      "learning_rate": 3.428070175438597e-05,
      "loss": 1.2214,
      "step": 28350
    },
    {
      "epoch": 8.304160269064853,
      "grad_norm": 1.6037787199020386,
      "learning_rate": 3.398830409356725e-05,
      "loss": 1.2208,
      "step": 28400
    },
    {
      "epoch": 8.31878335892374,
      "grad_norm": 1.3449786901474,
      "learning_rate": 3.369590643274854e-05,
      "loss": 1.2847,
      "step": 28450
    },
    {
      "epoch": 8.333406448782627,
      "grad_norm": 1.0352240800857544,
      "learning_rate": 3.3403508771929824e-05,
      "loss": 1.2672,
      "step": 28500
    },
    {
      "epoch": 8.348029538641516,
      "grad_norm": 0.813392162322998,
      "learning_rate": 3.311111111111112e-05,
      "loss": 1.2615,
      "step": 28550
    },
    {
      "epoch": 8.362652628500403,
      "grad_norm": 1.4502588510513306,
      "learning_rate": 3.28187134502924e-05,
      "loss": 1.2726,
      "step": 28600
    },
    {
      "epoch": 8.37727571835929,
      "grad_norm": 1.4803098440170288,
      "learning_rate": 3.2526315789473686e-05,
      "loss": 1.2238,
      "step": 28650
    },
    {
      "epoch": 8.391898808218176,
      "grad_norm": 0.955487847328186,
      "learning_rate": 3.223391812865497e-05,
      "loss": 1.3094,
      "step": 28700
    },
    {
      "epoch": 8.406521898077063,
      "grad_norm": 1.1420910358428955,
      "learning_rate": 3.194152046783626e-05,
      "loss": 1.3347,
      "step": 28750
    },
    {
      "epoch": 8.421144987935952,
      "grad_norm": 1.0753589868545532,
      "learning_rate": 3.164912280701755e-05,
      "loss": 1.2037,
      "step": 28800
    },
    {
      "epoch": 8.435768077794839,
      "grad_norm": 1.2044212818145752,
      "learning_rate": 3.1356725146198835e-05,
      "loss": 1.2662,
      "step": 28850
    },
    {
      "epoch": 8.450391167653725,
      "grad_norm": 1.0777748823165894,
      "learning_rate": 3.1064327485380115e-05,
      "loss": 1.298,
      "step": 28900
    },
    {
      "epoch": 8.465014257512612,
      "grad_norm": 1.4214991331100464,
      "learning_rate": 3.077192982456141e-05,
      "loss": 1.2135,
      "step": 28950
    },
    {
      "epoch": 8.479637347371499,
      "grad_norm": 1.2497597932815552,
      "learning_rate": 3.047953216374269e-05,
      "loss": 1.2407,
      "step": 29000
    },
    {
      "epoch": 8.494260437230388,
      "grad_norm": 1.4399263858795166,
      "learning_rate": 3.018713450292398e-05,
      "loss": 1.2739,
      "step": 29050
    },
    {
      "epoch": 8.508883527089274,
      "grad_norm": 1.2874088287353516,
      "learning_rate": 2.9894736842105264e-05,
      "loss": 1.2517,
      "step": 29100
    },
    {
      "epoch": 8.523506616948161,
      "grad_norm": 1.3855968713760376,
      "learning_rate": 2.9602339181286555e-05,
      "loss": 1.2299,
      "step": 29150
    },
    {
      "epoch": 8.538129706807048,
      "grad_norm": 1.2519357204437256,
      "learning_rate": 2.930994152046784e-05,
      "loss": 1.2247,
      "step": 29200
    },
    {
      "epoch": 8.552752796665935,
      "grad_norm": 0.9986673593521118,
      "learning_rate": 2.9023391812865498e-05,
      "loss": 1.262,
      "step": 29250
    },
    {
      "epoch": 8.567375886524824,
      "grad_norm": 1.288480520248413,
      "learning_rate": 2.8730994152046785e-05,
      "loss": 1.1731,
      "step": 29300
    },
    {
      "epoch": 8.58199897638371,
      "grad_norm": 1.138169765472412,
      "learning_rate": 2.843859649122807e-05,
      "loss": 1.2905,
      "step": 29350
    },
    {
      "epoch": 8.596622066242597,
      "grad_norm": 1.3479310274124146,
      "learning_rate": 2.814619883040936e-05,
      "loss": 1.2695,
      "step": 29400
    },
    {
      "epoch": 8.611245156101484,
      "grad_norm": 1.0346933603286743,
      "learning_rate": 2.7853801169590643e-05,
      "loss": 1.2825,
      "step": 29450
    },
    {
      "epoch": 8.62586824596037,
      "grad_norm": 0.9883565902709961,
      "learning_rate": 2.7561403508771934e-05,
      "loss": 1.3196,
      "step": 29500
    },
    {
      "epoch": 8.64049133581926,
      "grad_norm": 1.1008350849151611,
      "learning_rate": 2.7269005847953218e-05,
      "loss": 1.2254,
      "step": 29550
    },
    {
      "epoch": 8.655114425678146,
      "grad_norm": 1.0488311052322388,
      "learning_rate": 2.6976608187134505e-05,
      "loss": 1.2448,
      "step": 29600
    },
    {
      "epoch": 8.669737515537033,
      "grad_norm": 1.198706865310669,
      "learning_rate": 2.668421052631579e-05,
      "loss": 1.2308,
      "step": 29650
    },
    {
      "epoch": 8.68436060539592,
      "grad_norm": 0.7695217132568359,
      "learning_rate": 2.639181286549708e-05,
      "loss": 1.2358,
      "step": 29700
    },
    {
      "epoch": 8.698983695254807,
      "grad_norm": 1.2853639125823975,
      "learning_rate": 2.6099415204678363e-05,
      "loss": 1.217,
      "step": 29750
    },
    {
      "epoch": 8.713606785113695,
      "grad_norm": 1.5818493366241455,
      "learning_rate": 2.580701754385965e-05,
      "loss": 1.2288,
      "step": 29800
    },
    {
      "epoch": 8.728229874972582,
      "grad_norm": 1.2022409439086914,
      "learning_rate": 2.5514619883040934e-05,
      "loss": 1.2261,
      "step": 29850
    },
    {
      "epoch": 8.742852964831469,
      "grad_norm": 1.2274614572525024,
      "learning_rate": 2.5222222222222225e-05,
      "loss": 1.244,
      "step": 29900
    },
    {
      "epoch": 8.757476054690356,
      "grad_norm": 1.6010686159133911,
      "learning_rate": 2.492982456140351e-05,
      "loss": 1.2635,
      "step": 29950
    },
    {
      "epoch": 8.772099144549243,
      "grad_norm": 1.272159457206726,
      "learning_rate": 2.4637426900584796e-05,
      "loss": 1.266,
      "step": 30000
    },
    {
      "epoch": 8.786722234408131,
      "grad_norm": 1.423057198524475,
      "learning_rate": 2.4345029239766083e-05,
      "loss": 1.269,
      "step": 30050
    },
    {
      "epoch": 8.801345324267018,
      "grad_norm": 1.4131709337234497,
      "learning_rate": 2.4052631578947367e-05,
      "loss": 1.2979,
      "step": 30100
    },
    {
      "epoch": 8.815968414125905,
      "grad_norm": 1.33091402053833,
      "learning_rate": 2.3760233918128655e-05,
      "loss": 1.2551,
      "step": 30150
    },
    {
      "epoch": 8.830591503984792,
      "grad_norm": 1.4310269355773926,
      "learning_rate": 2.3467836257309942e-05,
      "loss": 1.2221,
      "step": 30200
    },
    {
      "epoch": 8.845214593843679,
      "grad_norm": 1.458746314048767,
      "learning_rate": 2.317543859649123e-05,
      "loss": 1.2722,
      "step": 30250
    },
    {
      "epoch": 8.859837683702565,
      "grad_norm": 1.0903661251068115,
      "learning_rate": 2.2883040935672513e-05,
      "loss": 1.2841,
      "step": 30300
    },
    {
      "epoch": 8.874460773561454,
      "grad_norm": 0.8985621333122253,
      "learning_rate": 2.25906432748538e-05,
      "loss": 1.2918,
      "step": 30350
    },
    {
      "epoch": 8.88908386342034,
      "grad_norm": 1.5443192720413208,
      "learning_rate": 2.2298245614035087e-05,
      "loss": 1.2448,
      "step": 30400
    },
    {
      "epoch": 8.903706953279228,
      "grad_norm": 1.0569283962249756,
      "learning_rate": 2.2005847953216375e-05,
      "loss": 1.2203,
      "step": 30450
    },
    {
      "epoch": 8.918330043138115,
      "grad_norm": 1.669854998588562,
      "learning_rate": 2.1713450292397662e-05,
      "loss": 1.1844,
      "step": 30500
    },
    {
      "epoch": 8.932953132997003,
      "grad_norm": 1.1618417501449585,
      "learning_rate": 2.1421052631578946e-05,
      "loss": 1.2929,
      "step": 30550
    },
    {
      "epoch": 8.94757622285589,
      "grad_norm": 1.1521883010864258,
      "learning_rate": 2.1128654970760233e-05,
      "loss": 1.2398,
      "step": 30600
    },
    {
      "epoch": 8.962199312714777,
      "grad_norm": 1.1815717220306396,
      "learning_rate": 2.083625730994152e-05,
      "loss": 1.3224,
      "step": 30650
    },
    {
      "epoch": 8.976822402573664,
      "grad_norm": 1.1747978925704956,
      "learning_rate": 2.0543859649122807e-05,
      "loss": 1.2639,
      "step": 30700
    },
    {
      "epoch": 8.99144549243255,
      "grad_norm": 1.3548763990402222,
      "learning_rate": 2.0251461988304095e-05,
      "loss": 1.3563,
      "step": 30750
    },
    {
      "epoch": 9.005849235943554,
      "grad_norm": 1.1407395601272583,
      "learning_rate": 1.995906432748538e-05,
      "loss": 1.2007,
      "step": 30800
    },
    {
      "epoch": 9.020472325802443,
      "grad_norm": 1.0930087566375732,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 1.2801,
      "step": 30850
    },
    {
      "epoch": 9.03509541566133,
      "grad_norm": 0.8455339074134827,
      "learning_rate": 1.9374269005847953e-05,
      "loss": 1.2935,
      "step": 30900
    },
    {
      "epoch": 9.049718505520216,
      "grad_norm": 1.3221628665924072,
      "learning_rate": 1.908187134502924e-05,
      "loss": 1.2142,
      "step": 30950
    },
    {
      "epoch": 9.064341595379103,
      "grad_norm": 1.1435519456863403,
      "learning_rate": 1.8789473684210528e-05,
      "loss": 1.1879,
      "step": 31000
    },
    {
      "epoch": 9.07896468523799,
      "grad_norm": 1.2936042547225952,
      "learning_rate": 1.849707602339181e-05,
      "loss": 1.2494,
      "step": 31050
    },
    {
      "epoch": 9.093587775096879,
      "grad_norm": 1.469067931175232,
      "learning_rate": 1.82046783625731e-05,
      "loss": 1.216,
      "step": 31100
    },
    {
      "epoch": 9.108210864955765,
      "grad_norm": 1.0376588106155396,
      "learning_rate": 1.7912280701754386e-05,
      "loss": 1.2489,
      "step": 31150
    },
    {
      "epoch": 9.122833954814652,
      "grad_norm": 1.481278657913208,
      "learning_rate": 1.7619883040935673e-05,
      "loss": 1.3041,
      "step": 31200
    },
    {
      "epoch": 9.137457044673539,
      "grad_norm": 1.0595550537109375,
      "learning_rate": 1.732748538011696e-05,
      "loss": 1.1466,
      "step": 31250
    },
    {
      "epoch": 9.152080134532426,
      "grad_norm": 1.2450391054153442,
      "learning_rate": 1.7035087719298244e-05,
      "loss": 1.2934,
      "step": 31300
    },
    {
      "epoch": 9.166703224391314,
      "grad_norm": 1.1343196630477905,
      "learning_rate": 1.674269005847953e-05,
      "loss": 1.2983,
      "step": 31350
    },
    {
      "epoch": 9.181326314250201,
      "grad_norm": 1.209010362625122,
      "learning_rate": 1.645029239766082e-05,
      "loss": 1.1791,
      "step": 31400
    },
    {
      "epoch": 9.195949404109088,
      "grad_norm": 1.3471565246582031,
      "learning_rate": 1.6157894736842106e-05,
      "loss": 1.1565,
      "step": 31450
    },
    {
      "epoch": 9.210572493967975,
      "grad_norm": 0.6219309568405151,
      "learning_rate": 1.5865497076023393e-05,
      "loss": 1.2382,
      "step": 31500
    },
    {
      "epoch": 9.225195583826862,
      "grad_norm": 1.2614822387695312,
      "learning_rate": 1.5573099415204677e-05,
      "loss": 1.2797,
      "step": 31550
    },
    {
      "epoch": 9.23981867368575,
      "grad_norm": 1.3172229528427124,
      "learning_rate": 1.5280701754385964e-05,
      "loss": 1.2427,
      "step": 31600
    },
    {
      "epoch": 9.254441763544637,
      "grad_norm": 0.8178282976150513,
      "learning_rate": 1.4988304093567252e-05,
      "loss": 1.3494,
      "step": 31650
    },
    {
      "epoch": 9.269064853403524,
      "grad_norm": 1.2040928602218628,
      "learning_rate": 1.4695906432748537e-05,
      "loss": 1.2083,
      "step": 31700
    },
    {
      "epoch": 9.28368794326241,
      "grad_norm": 1.2066682577133179,
      "learning_rate": 1.4403508771929824e-05,
      "loss": 1.3037,
      "step": 31750
    },
    {
      "epoch": 9.298311033121298,
      "grad_norm": 1.2526495456695557,
      "learning_rate": 1.4111111111111112e-05,
      "loss": 1.231,
      "step": 31800
    },
    {
      "epoch": 9.312934122980186,
      "grad_norm": 1.2157269716262817,
      "learning_rate": 1.3818713450292397e-05,
      "loss": 1.2381,
      "step": 31850
    },
    {
      "epoch": 9.327557212839073,
      "grad_norm": 1.1095521450042725,
      "learning_rate": 1.3526315789473685e-05,
      "loss": 1.2777,
      "step": 31900
    },
    {
      "epoch": 9.34218030269796,
      "grad_norm": 1.2745728492736816,
      "learning_rate": 1.323391812865497e-05,
      "loss": 1.2495,
      "step": 31950
    },
    {
      "epoch": 9.356803392556847,
      "grad_norm": 1.3394304513931274,
      "learning_rate": 1.2941520467836257e-05,
      "loss": 1.2278,
      "step": 32000
    },
    {
      "epoch": 9.371426482415734,
      "grad_norm": 1.1142871379852295,
      "learning_rate": 1.2649122807017545e-05,
      "loss": 1.2255,
      "step": 32050
    },
    {
      "epoch": 9.386049572274622,
      "grad_norm": 1.4370290040969849,
      "learning_rate": 1.235672514619883e-05,
      "loss": 1.3167,
      "step": 32100
    },
    {
      "epoch": 9.400672662133509,
      "grad_norm": 1.1707899570465088,
      "learning_rate": 1.2064327485380117e-05,
      "loss": 1.2191,
      "step": 32150
    },
    {
      "epoch": 9.415295751992396,
      "grad_norm": 0.9688663482666016,
      "learning_rate": 1.1771929824561403e-05,
      "loss": 1.2645,
      "step": 32200
    },
    {
      "epoch": 9.429918841851283,
      "grad_norm": 1.0472708940505981,
      "learning_rate": 1.147953216374269e-05,
      "loss": 1.2145,
      "step": 32250
    },
    {
      "epoch": 9.44454193171017,
      "grad_norm": 1.230147123336792,
      "learning_rate": 1.1187134502923976e-05,
      "loss": 1.2215,
      "step": 32300
    },
    {
      "epoch": 9.459165021569058,
      "grad_norm": 1.2899903059005737,
      "learning_rate": 1.0894736842105263e-05,
      "loss": 1.1616,
      "step": 32350
    },
    {
      "epoch": 9.473788111427945,
      "grad_norm": 1.2619802951812744,
      "learning_rate": 1.060233918128655e-05,
      "loss": 1.1555,
      "step": 32400
    },
    {
      "epoch": 9.488411201286832,
      "grad_norm": 1.183584451675415,
      "learning_rate": 1.0309941520467836e-05,
      "loss": 1.2258,
      "step": 32450
    },
    {
      "epoch": 9.503034291145719,
      "grad_norm": 1.1350370645523071,
      "learning_rate": 1.0017543859649123e-05,
      "loss": 1.2333,
      "step": 32500
    },
    {
      "epoch": 9.517657381004605,
      "grad_norm": 1.7028076648712158,
      "learning_rate": 9.725146198830409e-06,
      "loss": 1.2407,
      "step": 32550
    },
    {
      "epoch": 9.532280470863494,
      "grad_norm": 1.040753722190857,
      "learning_rate": 9.432748538011696e-06,
      "loss": 1.2681,
      "step": 32600
    },
    {
      "epoch": 9.54690356072238,
      "grad_norm": 1.0960992574691772,
      "learning_rate": 9.140350877192983e-06,
      "loss": 1.2906,
      "step": 32650
    },
    {
      "epoch": 9.561526650581268,
      "grad_norm": 1.286303997039795,
      "learning_rate": 8.847953216374269e-06,
      "loss": 1.1933,
      "step": 32700
    },
    {
      "epoch": 9.576149740440155,
      "grad_norm": 1.282431960105896,
      "learning_rate": 8.555555555555556e-06,
      "loss": 1.2458,
      "step": 32750
    },
    {
      "epoch": 9.590772830299041,
      "grad_norm": 1.193554162979126,
      "learning_rate": 8.263157894736841e-06,
      "loss": 1.2118,
      "step": 32800
    },
    {
      "epoch": 9.60539592015793,
      "grad_norm": 1.2011229991912842,
      "learning_rate": 7.970760233918129e-06,
      "loss": 1.2635,
      "step": 32850
    },
    {
      "epoch": 9.620019010016817,
      "grad_norm": 1.4167327880859375,
      "learning_rate": 7.678362573099416e-06,
      "loss": 1.1887,
      "step": 32900
    },
    {
      "epoch": 9.634642099875704,
      "grad_norm": 1.2262492179870605,
      "learning_rate": 7.3859649122807015e-06,
      "loss": 1.2802,
      "step": 32950
    },
    {
      "epoch": 9.64926518973459,
      "grad_norm": 1.3224639892578125,
      "learning_rate": 7.093567251461988e-06,
      "loss": 1.2011,
      "step": 33000
    },
    {
      "epoch": 9.663888279593477,
      "grad_norm": 1.0829658508300781,
      "learning_rate": 6.801169590643275e-06,
      "loss": 1.2068,
      "step": 33050
    },
    {
      "epoch": 9.678511369452366,
      "grad_norm": 1.087600827217102,
      "learning_rate": 6.5087719298245616e-06,
      "loss": 1.2366,
      "step": 33100
    },
    {
      "epoch": 9.693134459311253,
      "grad_norm": 1.3499724864959717,
      "learning_rate": 6.216374269005848e-06,
      "loss": 1.2661,
      "step": 33150
    },
    {
      "epoch": 9.70775754917014,
      "grad_norm": 1.155164361000061,
      "learning_rate": 5.923976608187134e-06,
      "loss": 1.2937,
      "step": 33200
    },
    {
      "epoch": 9.722380639029026,
      "grad_norm": 1.502790927886963,
      "learning_rate": 5.631578947368421e-06,
      "loss": 1.2446,
      "step": 33250
    },
    {
      "epoch": 9.737003728887913,
      "grad_norm": 1.1834588050842285,
      "learning_rate": 5.345029239766082e-06,
      "loss": 1.2661,
      "step": 33300
    },
    {
      "epoch": 9.751626818746802,
      "grad_norm": 1.3259260654449463,
      "learning_rate": 5.052631578947369e-06,
      "loss": 1.2508,
      "step": 33350
    },
    {
      "epoch": 9.766249908605689,
      "grad_norm": 1.2783772945404053,
      "learning_rate": 4.760233918128655e-06,
      "loss": 1.2753,
      "step": 33400
    },
    {
      "epoch": 9.780872998464575,
      "grad_norm": 1.1759463548660278,
      "learning_rate": 4.4678362573099416e-06,
      "loss": 1.2264,
      "step": 33450
    },
    {
      "epoch": 9.795496088323462,
      "grad_norm": 1.0571049451828003,
      "learning_rate": 4.175438596491228e-06,
      "loss": 1.2094,
      "step": 33500
    },
    {
      "epoch": 9.81011917818235,
      "grad_norm": 1.387675166130066,
      "learning_rate": 3.883040935672514e-06,
      "loss": 1.1989,
      "step": 33550
    },
    {
      "epoch": 9.824742268041238,
      "grad_norm": 0.8371745347976685,
      "learning_rate": 3.590643274853801e-06,
      "loss": 1.3349,
      "step": 33600
    },
    {
      "epoch": 9.839365357900125,
      "grad_norm": 1.3681576251983643,
      "learning_rate": 3.298245614035088e-06,
      "loss": 1.2737,
      "step": 33650
    },
    {
      "epoch": 9.853988447759011,
      "grad_norm": 0.9340406060218811,
      "learning_rate": 3.0058479532163744e-06,
      "loss": 1.2254,
      "step": 33700
    },
    {
      "epoch": 9.868611537617898,
      "grad_norm": 1.1671795845031738,
      "learning_rate": 2.713450292397661e-06,
      "loss": 1.2633,
      "step": 33750
    },
    {
      "epoch": 9.883234627476785,
      "grad_norm": 1.4941751956939697,
      "learning_rate": 2.4210526315789477e-06,
      "loss": 1.2397,
      "step": 33800
    },
    {
      "epoch": 9.897857717335674,
      "grad_norm": 0.9127883911132812,
      "learning_rate": 2.128654970760234e-06,
      "loss": 1.2328,
      "step": 33850
    },
    {
      "epoch": 9.91248080719456,
      "grad_norm": 1.122633457183838,
      "learning_rate": 1.8362573099415207e-06,
      "loss": 1.2242,
      "step": 33900
    },
    {
      "epoch": 9.927103897053447,
      "grad_norm": 1.1643903255462646,
      "learning_rate": 1.543859649122807e-06,
      "loss": 1.2646,
      "step": 33950
    },
    {
      "epoch": 9.941726986912334,
      "grad_norm": 1.057323694229126,
      "learning_rate": 1.2514619883040935e-06,
      "loss": 1.2752,
      "step": 34000
    },
    {
      "epoch": 9.956350076771221,
      "grad_norm": 1.4964274168014526,
      "learning_rate": 9.5906432748538e-07,
      "loss": 1.2777,
      "step": 34050
    },
    {
      "epoch": 9.97097316663011,
      "grad_norm": 1.1029702425003052,
      "learning_rate": 6.666666666666667e-07,
      "loss": 1.2527,
      "step": 34100
    },
    {
      "epoch": 9.985596256488996,
      "grad_norm": 1.3115006685256958,
      "learning_rate": 3.7426900584795327e-07,
      "loss": 1.2331,
      "step": 34150
    },
    {
      "epoch": 10.0,
      "grad_norm": 2.248868465423584,
      "learning_rate": 8.187134502923978e-08,
      "loss": 1.2953,
      "step": 34200
    }
  ],
  "logging_steps": 50,
  "max_steps": 34200,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8.702626805003059e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
